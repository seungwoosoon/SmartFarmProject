{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI7VBrgqgM8LMeAnpRGr0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungwoosoon/SmartFarmProject/blob/AI/predict_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm_l47uXLFVv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° í´ë”\n",
        "import os\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/mod'\n",
        "files_in_directory = os.listdir(directory_path)\n",
        "print(files_in_directory)"
      ],
      "metadata": {
        "id": "ULJTr6i7LHnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gr1 = pd.read_excel('/content/drive/MyDrive/mod/gr1.xlsx')\n",
        "df_en1 = pd.read_excel('/content/drive/MyDrive/mod/en1.xlsx')\n",
        "display(df_gr1.head())\n",
        "display(df_en1.head())"
      ],
      "metadata": {
        "id": "jcvkIOpgLUY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_gr1.head())\n",
        "display(df_en1.head())\n",
        "\n",
        "print(df_gr1.columns)\n",
        "print(df_en1.columns)\n",
        "\n",
        "df_gr1_numeric = df_gr1.apply(pd.to_numeric, errors='coerce')\n",
        "df_en1_numeric = df_en1.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "df_gr1_numeric.dropna(axis=1, how='all', inplace=True)\n",
        "df_en1_numeric.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "display(df_gr1_numeric.head())\n",
        "display(df_en1_numeric.head())\n",
        "\n",
        "print(df_gr1_numeric.columns)\n",
        "print(df_en1_numeric.columns)"
      ],
      "metadata": {
        "id": "L3N2erWeLbml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with all zero values\n",
        "df_gr1_numeric = df_gr1_numeric.loc[:, (df_gr1_numeric != 0).any(axis=0)]\n",
        "df_en1_numeric = df_en1_numeric.loc[:, (df_en1_numeric != 0).any(axis=0)]\n",
        "\n",
        "df_en1_numeric = df_en1_numeric.reset_index(drop=True)\n",
        "df_gr1_numeric = df_gr1_numeric.reset_index(drop=True)\n",
        "\n",
        "merged_df = pd.concat([df_gr1_numeric, df_en1_numeric], axis=1)\n",
        "\n",
        "merged_df = merged_df.drop('ì£¼ì°¨', axis=1)\n",
        "\n",
        "correlation_matrix = merged_df.corr()\n",
        "display(correlation_matrix.head())\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reset font to default for English characters\n",
        "plt.rcParams['font.family'] = plt.rcParamsDefault['font.family']\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Growth and Environment Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-vxtBbQ0Lk6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ˆê¸° ë„ì „\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ”¥ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "class SmartFarmGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes=[64, 32], dense_sizes=[16],\n",
        "                 output_size=2, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        ìŠ¤ë§ˆíŠ¸íŒœ GRU ëª¨ë¸\n",
        "\n",
        "        Args:\n",
        "            input_size: ì…ë ¥ íŠ¹ì„± ìˆ˜ (ì˜ˆ: 23)\n",
        "            hidden_sizes: GRU ë ˆì´ì–´ íˆë“  ìœ ë‹› ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
        "            dense_sizes: Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
        "            output_size: ì¶œë ¥ í¬ê¸° (íƒ€ê²Ÿ ìˆ˜, ì˜ˆ: 2)\n",
        "            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "        \"\"\"\n",
        "        super(SmartFarmGRU, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = len(hidden_sizes)\n",
        "\n",
        "        # GRU ë ˆì´ì–´ë“¤\n",
        "        self.gru_layers = nn.ModuleList()\n",
        "\n",
        "        # ì²« ë²ˆì§¸ GRU ë ˆì´ì–´\n",
        "        self.gru_layers.append(\n",
        "            nn.GRU(input_size, hidden_sizes[0], batch_first=True, dropout=dropout_rate)\n",
        "        )\n",
        "\n",
        "        # ì¶”ê°€ GRU ë ˆì´ì–´ë“¤\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.gru_layers.append(\n",
        "                nn.GRU(hidden_sizes[i-1], hidden_sizes[i], batch_first=True, dropout=dropout_rate)\n",
        "            )\n",
        "\n",
        "        # BatchNorm ë ˆì´ì–´ë“¤\n",
        "        self.batch_norms = nn.ModuleList([\n",
        "            nn.BatchNorm1d(size) for size in hidden_sizes\n",
        "        ])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Dense ë ˆì´ì–´ë“¤\n",
        "        self.dense_layers = nn.ModuleList()\n",
        "\n",
        "        # ì²« ë²ˆì§¸ Dense ë ˆì´ì–´ (GRU â†’ Dense)\n",
        "        if dense_sizes:\n",
        "            self.dense_layers.append(nn.Linear(hidden_sizes[-1], dense_sizes[0]))\n",
        "            self.dense_batch_norms = nn.ModuleList([nn.BatchNorm1d(dense_sizes[0])])\n",
        "\n",
        "            # ì¶”ê°€ Dense ë ˆì´ì–´ë“¤\n",
        "            for i in range(1, len(dense_sizes)):\n",
        "                self.dense_layers.append(nn.Linear(dense_sizes[i-1], dense_sizes[i]))\n",
        "                self.dense_batch_norms.append(nn.BatchNorm1d(dense_sizes[i]))\n",
        "\n",
        "            # ì¶œë ¥ ë ˆì´ì–´\n",
        "            self.output_layer = nn.Linear(dense_sizes[-1], output_size)\n",
        "        else:\n",
        "            # Dense ë ˆì´ì–´ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì¶œë ¥\n",
        "            self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "            self.dense_batch_norms = nn.ModuleList()\n",
        "\n",
        "        # í™œì„±í™” í•¨ìˆ˜\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "\n",
        "        # GRU ë ˆì´ì–´ë“¤ í†µê³¼\n",
        "        for i, gru_layer in enumerate(self.gru_layers):\n",
        "            x, _ = gru_layer(x)\n",
        "\n",
        "            # ë§ˆì§€ë§‰ GRU ë ˆì´ì–´ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ BatchNorm ì ìš©\n",
        "            if i < len(self.gru_layers) - 1:\n",
        "                # x shape: (batch_size, seq_len, hidden_size)\n",
        "                x = x.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
        "                x = self.batch_norms[i](x)\n",
        "                x = x.transpose(1, 2)  # (batch_size, seq_len, hidden_size)\n",
        "                x = self.dropout(x)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©\n",
        "        x = x[:, -1, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Dense ë ˆì´ì–´ë“¤ í†µê³¼\n",
        "        for i, dense_layer in enumerate(self.dense_layers):\n",
        "            x = dense_layer(x)\n",
        "            x = self.dense_batch_norms[i](x)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # ì¶œë ¥ ë ˆì´ì–´\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SmartFarmTrainer:\n",
        "    def __init__(self, model, target_names=['leaf_number', 'growth_length']):\n",
        "        \"\"\"\n",
        "        ìŠ¤ë§ˆíŠ¸íŒœ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch ëª¨ë¸\n",
        "            target_names: íƒ€ê²Ÿ ë³€ìˆ˜ ì´ë¦„ë“¤\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.target_names = target_names\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_maes = []\n",
        "        self.val_maes = []\n",
        "\n",
        "    def train_epoch(self, train_loader, optimizer, criterion):\n",
        "        \"\"\"í•œ ì—í¬í¬ í›ˆë ¨\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_mae = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "            total_loss += loss.item()\n",
        "            mae = torch.mean(torch.abs(outputs - batch_y)).item()\n",
        "            total_mae += mae\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_mae = total_mae / len(train_loader)\n",
        "\n",
        "        return avg_loss, avg_mae\n",
        "\n",
        "    def validate_epoch(self, val_loader, criterion):\n",
        "        \"\"\"í•œ ì—í¬í¬ ê²€ì¦\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_mae = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                mae = torch.mean(torch.abs(outputs - batch_y)).item()\n",
        "                total_mae += mae\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        avg_mae = total_mae / len(val_loader)\n",
        "\n",
        "        return avg_loss, avg_mae\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val,\n",
        "              epochs=100, batch_size=32, learning_rate=0.001,\n",
        "              patience=15, save_path='/content/drive/MyDrive/mod/processed/best_model.pth'):\n",
        "        \"\"\"\n",
        "        ëª¨ë¸ í›ˆë ¨\n",
        "\n",
        "        Args:\n",
        "            X_train, y_train: í›ˆë ¨ ë°ì´í„°\n",
        "            X_val, y_val: ê²€ì¦ ë°ì´í„°\n",
        "            epochs: ì—í¬í¬ ìˆ˜\n",
        "            batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "            learning_rate: í•™ìŠµë¥ \n",
        "            patience: ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬\n",
        "            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        print(f\"ğŸš€ PyTorch GRU ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "        print(f\"   í›ˆë ¨ ë°ì´í„°: {X_train.shape}\")\n",
        "        print(f\"   ê²€ì¦ ë°ì´í„°: {X_val.shape}\")\n",
        "\n",
        "        # ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
        "        X_train_tensor = torch.FloatTensor(X_train)\n",
        "        y_train_tensor = torch.FloatTensor(y_train)\n",
        "        X_val_tensor = torch.FloatTensor(X_val)\n",
        "        y_val_tensor = torch.FloatTensor(y_val)\n",
        "\n",
        "        # ë°ì´í„° ë¡œë” ìƒì„±\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "        criterion = nn.MSELoss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
        "        )\n",
        "\n",
        "        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        # í›ˆë ¨ ë£¨í”„\n",
        "        for epoch in range(epochs):\n",
        "            # í›ˆë ¨\n",
        "            train_loss, train_mae = self.train_epoch(train_loader, optimizer, criterion)\n",
        "\n",
        "            # ê²€ì¦\n",
        "            val_loss, val_mae = self.validate_epoch(val_loader, criterion)\n",
        "\n",
        "            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_maes.append(train_mae)\n",
        "            self.val_maes.append(val_mae)\n",
        "\n",
        "            # ë¡œê·¸ ì¶œë ¥\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "                print(f'  Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f}')\n",
        "                print(f'  Val Loss: {val_loss:.6f}, Val MAE: {val_mae:.6f}')\n",
        "                print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.8f}')\n",
        "                print()\n",
        "\n",
        "            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # ìµœê³  ëª¨ë¸ ì €ì¥\n",
        "                torch.save(self.model.state_dict(), save_path)\n",
        "                print(f'ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {val_loss:.6f}')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
        "                break\n",
        "\n",
        "        # ìµœê³  ëª¨ë¸ ë¡œë“œ\n",
        "        self.model.load_state_dict(torch.load(save_path))\n",
        "        print(\"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œë¨\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def evaluate(self, X_test, y_test, scaler_growth):\n",
        "        \"\"\"\n",
        "        ëª¨ë¸ í‰ê°€\n",
        "\n",
        "        Args:\n",
        "            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
        "            scaler_growth: íƒ€ê²Ÿ ë³€ìˆ˜ ìŠ¤ì¼€ì¼ëŸ¬\n",
        "        \"\"\"\n",
        "        print(\"ğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred = self.model(X_test_tensor).cpu().numpy()\n",
        "\n",
        "        # ì •ê·œí™” í•´ì œ\n",
        "        y_test_original = scaler_growth.inverse_transform(y_test)\n",
        "        y_pred_original = scaler_growth.inverse_transform(y_pred)\n",
        "\n",
        "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "        results = {}\n",
        "        for i, target_name in enumerate(self.target_names):\n",
        "            mse = mean_squared_error(y_test_original[:, i], y_pred_original[:, i])\n",
        "            mae = mean_absolute_error(y_test_original[:, i], y_pred_original[:, i])\n",
        "            r2 = r2_score(y_test_original[:, i], y_pred_original[:, i])\n",
        "\n",
        "            results[target_name] = {\n",
        "                'MSE': mse,\n",
        "                'RMSE': np.sqrt(mse),\n",
        "                'MAE': mae,\n",
        "                'RÂ²': r2\n",
        "            }\n",
        "\n",
        "            print(f\"\\nğŸ“Š {target_name} ì„±ëŠ¥:\")\n",
        "            print(f\"   RMSE: {np.sqrt(mse):.4f}\")\n",
        "            print(f\"   MAE:  {mae:.4f}\")\n",
        "            print(f\"   RÂ²:   {r2:.4f}\")\n",
        "\n",
        "        return results, y_pred_original, y_test_original\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss ê·¸ë˜í”„\n",
        "        axes[0].plot(self.train_losses, label='í›ˆë ¨ ì†ì‹¤', color='blue')\n",
        "        axes[0].plot(self.val_losses, label='ê²€ì¦ ì†ì‹¤', color='orange')\n",
        "        axes[0].set_title('ëª¨ë¸ ì†ì‹¤')\n",
        "        axes[0].set_xlabel('ì—í¬í¬')\n",
        "        axes[0].set_ylabel('ì†ì‹¤')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # MAE ê·¸ë˜í”„\n",
        "        axes[1].plot(self.train_maes, label='í›ˆë ¨ MAE', color='blue')\n",
        "        axes[1].plot(self.val_maes, label='ê²€ì¦ MAE', color='orange')\n",
        "        axes[1].set_title('ëª¨ë¸ MAE')\n",
        "        axes[1].set_xlabel('ì—í¬í¬')\n",
        "        axes[1].set_ylabel('MAE')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_predictions(self, y_true, y_pred, target_names=None):\n",
        "        \"\"\"ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\"\"\"\n",
        "        if target_names is None:\n",
        "            target_names = ['ì—½ìˆ˜', 'ìƒì¥ê¸¸ì´'] # í•œê¸€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
        "\n",
        "        fig, axes = plt.subplots(1, len(target_names), figsize=(6*len(target_names), 5))\n",
        "        if len(target_names) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, target_name in enumerate(target_names):\n",
        "            # ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„\n",
        "            axes[i].scatter(y_true[:, i], y_pred[:, i], alpha=0.6, color='blue')\n",
        "\n",
        "            # ì™„ë²½í•œ ì˜ˆì¸¡ì„  (y=x)\n",
        "            min_val = min(y_true[:, i].min(), y_pred[:, i].min())\n",
        "            max_val = max(y_true[:, i].max(), y_pred[:, i].max())\n",
        "            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ì™„ë²½ ì˜ˆì¸¡')\n",
        "\n",
        "            axes[i].set_xlabel(f'ì‹¤ì œ {target_name}')\n",
        "            axes[i].set_ylabel(f'ì˜ˆì¸¡ {target_name}')\n",
        "            axes[i].set_title(f'{target_name} ì˜ˆì¸¡ ê²°ê³¼')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "            # RÂ² ê°’ í‘œì‹œ\n",
        "            r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
        "            axes[i].text(0.05, 0.95, f'RÂ² = {r2:.3f}',\n",
        "                        transform=axes[i].transAxes,\n",
        "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_time_series_predictions(self, y_true, y_pred, target_names=None, sample_size=100):\n",
        "        \"\"\"ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\"\"\"\n",
        "        if target_names is None:\n",
        "            target_names = ['ì—½ìˆ˜', 'ìƒì¥ê¸¸ì´'] # í•œê¸€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
        "\n",
        "        # ìƒ˜í”Œ í¬ê¸° ì¡°ì •\n",
        "        n_samples = min(sample_size, len(y_true))\n",
        "        indices = np.random.choice(len(y_true), n_samples, replace=False)\n",
        "        indices = np.sort(indices)\n",
        "\n",
        "        fig, axes = plt.subplots(len(target_names), 1, figsize=(12, 4*len(target_names)))\n",
        "        if len(target_names) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, target_name in enumerate(target_names):\n",
        "            axes[i].plot(indices, y_true[indices, i], 'o-', label='ì‹¤ì œê°’', alpha=0.7, color='blue')\n",
        "            axes[i].plot(indices, y_pred[indices, i], 's-', label='ì˜ˆì¸¡ê°’', alpha=0.7, color='orange')\n",
        "            axes[i].set_title(f'{target_name} ì‹œê³„ì—´ ì˜ˆì¸¡')\n",
        "            axes[i].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤')\n",
        "            axes[i].set_ylabel(target_name)\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main_pytorch_training(processor, train_data, val_data, test_data):\n",
        "    \"\"\"ë©”ì¸ PyTorch ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜\"\"\"\n",
        "    print(\"ğŸ”¥ ìŠ¤ë§ˆíŠ¸íŒœ PyTorch GRU ëª¨ë¸ í›ˆë ¨ ì‹œì‘\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ë°ì´í„° ì¶”ì¶œ\n",
        "    X_train, y_train = train_data\n",
        "    X_val, y_val = val_data\n",
        "    X_test, y_test = test_data\n",
        "\n",
        "    if X_train is None:\n",
        "        print(\"âŒ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"ğŸ“Š ë°ì´í„° í¬ê¸°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    model = SmartFarmGRU(\n",
        "        input_size=X_train.shape[2],  # íŠ¹ì„± ìˆ˜ (ì˜ˆ: 23)\n",
        "        hidden_sizes=[64, 32],\n",
        "        dense_sizes=[16],\n",
        "        output_size=y_train.shape[1],  # íƒ€ê²Ÿ ìˆ˜ (ì˜ˆ: 2)\n",
        "        dropout_rate=0.2\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ¤– ëª¨ë¸ êµ¬ì¡°:\")\n",
        "    print(f\"   ì…ë ¥ í¬ê¸°: {X_train.shape[2]}\")\n",
        "    print(f\"   GRU ë ˆì´ì–´: {[64, 32]}\")\n",
        "    print(f\"   Dense ë ˆì´ì–´: {[16]}\")\n",
        "    print(f\"   ì¶œë ¥ í¬ê¸°: {y_train.shape[1]}\")\n",
        "    print(f\"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
        "    trainer = SmartFarmTrainer(model, target_names=['leaf_number', 'growth_length']) # target_namesëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "    # ëª¨ë¸ í›ˆë ¨\n",
        "    train_losses, val_losses = trainer.train(\n",
        "        X_train, y_train,\n",
        "        X_val, y_val,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        learning_rate=0.001,\n",
        "        patience=15\n",
        "    )\n",
        "\n",
        "    # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\n",
        "    trainer.plot_training_history()\n",
        "\n",
        "    # ëª¨ë¸ í‰ê°€\n",
        "    results, y_pred, y_true = trainer.evaluate(X_test, y_test, processor.scaler_growth)\n",
        "\n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” (í•œê¸€ ë ˆì´ë¸” ì ìš©)\n",
        "    trainer.plot_predictions(y_true, y_pred)\n",
        "    trainer.plot_time_series_predictions(y_true, y_pred, sample_size=50)\n",
        "\n",
        "    print(\"\\nğŸ‰ PyTorch ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ì™„ë£Œ!\")\n",
        "    print(\"\\nğŸ“ˆ ìµœì¢… ì„±ëŠ¥ ìš”ì•½:\")\n",
        "    for target_name, metrics in results.items():\n",
        "        print(f\"  {target_name} ({'ì—½ìˆ˜' if target_name == 'leaf_number' else 'ìƒì¥ê¸¸ì´'}):\")\n",
        "        print(f\"    RMSE: {metrics['RMSE']:.4f}\")\n",
        "        print(f\"    RÂ²:   {metrics['RÂ²']:.4f}\")\n",
        "\n",
        "    return model, trainer, results\n",
        "\n",
        "# ì‹¤í–‰ ì˜ˆì œ\n",
        "if __name__ == \"__main__\":\n",
        "    # ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ìƒíƒœì—ì„œ ì‹¤í–‰\n",
        "    # processor, train_data, val_data, test_data = main()  # ì „ì²˜ë¦¬ ì½”ë“œ ì‹¤í–‰\n",
        "\n",
        "    # PyTorch ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
        "    # model, trainer, results = main_pytorch_training(processor, train_data, val_data, test_data)\n",
        "\n",
        "    print(\"ğŸ”¥ PyTorch GRU ëª¨ë¸ ì½”ë“œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(\"ğŸ“ ì‚¬ìš©ë²•:\")\n",
        "    print(\"   1. ì „ì²˜ë¦¬ ì½”ë“œ ì‹¤í–‰: processor, train_data, val_data, test_data = main()\")\n",
        "    print(\"   2. PyTorch ëª¨ë¸ í›ˆë ¨: model, trainer, results = main_pytorch_training(processor, train_data, val_data, test_data)\")"
      ],
      "metadata": {
        "id": "QhJG9MvDLxT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì „ì²˜ë¦¬ (ì´ë¯¸ í–ˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)\n",
        "print(\"1ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰...\")\n",
        "processor, train_data, val_data, test_data = main()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"2ï¸âƒ£ PyTorch GRU ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "\n",
        "# 2. PyTorch ëª¨ë¸ í›ˆë ¨\n",
        "model, trainer, results = main_pytorch_training(processor, train_data, val_data, test_data)\n",
        "\n",
        "print(\"\\nğŸ‰ ëª¨ë“  ê³¼ì • ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "M3Cydr4IL1dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì „ì²˜ë¦¬ ë””ë²„ê¹… ë° ìˆ˜ì • ì½”ë“œ\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def debug_preprocessing_issues(processor):\n",
        "    \"\"\"ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë°œìƒí•œ ë¬¸ì œë“¤ì„ ë””ë²„ê¹…\"\"\"\n",
        "\n",
        "    print(\"ğŸ” ì „ì²˜ë¦¬ ë””ë²„ê¹… ì‹œì‘\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. ë³‘í•©ëœ ë°ì´í„° í™•ì¸\n",
        "    if hasattr(processor, 'merged_data') and processor.merged_data is not None:\n",
        "        print(f\"ğŸ“Š ë³‘í•©ëœ ë°ì´í„° í¬ê¸°: {processor.merged_data.shape}\")\n",
        "        print(f\"ğŸ“‹ ì „ì²´ ì»¬ëŸ¼ ëª©ë¡:\")\n",
        "        for i, col in enumerate(processor.merged_data.columns):\n",
        "            print(f\"  {i+1:2d}. {col}\")\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
        "        env_cols = [col for col in processor.merged_data.columns\n",
        "                   if col.startswith(('internal_', 'external_'))]\n",
        "        print(f\"\\nğŸŒ¡ï¸ í™˜ê²½ ë³€ìˆ˜ ({len(env_cols)}ê°œ): {env_cols}\")\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
        "        target_cols = [col for col in processor.merged_data.columns\n",
        "                      if col in ['leaf_number', 'growth_length']]\n",
        "        print(f\"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ({len(target_cols)}ê°œ): {target_cols}\")\n",
        "\n",
        "        # ì§€ì—° íŠ¹ì„± í™•ì¸\n",
        "        lag_cols = [col for col in processor.merged_data.columns if 'lag' in col]\n",
        "        print(f\"â° ì§€ì—° íŠ¹ì„± ({len(lag_cols)}ê°œ): {lag_cols}\")\n",
        "\n",
        "        # íŒŒìƒ íŠ¹ì„± í™•ì¸\n",
        "        derived_cols = [col for col in processor.merged_data.columns\n",
        "                       if any(x in col for x in ['temp_diff', 'solar_efficiency', 'temp_humidity_index', 'week_sin', 'week_cos'])]\n",
        "        print(f\"ğŸ”§ íŒŒìƒ íŠ¹ì„± ({len(derived_cols)}ê°œ): {derived_cols}\")\n",
        "\n",
        "        # ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
        "        print(f\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ:\")\n",
        "        print(processor.merged_data.head(3))\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ ë³‘í•©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "    return\n",
        "\n",
        "def fix_preprocessing_pipeline():\n",
        "    \"\"\"ìˆ˜ì •ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
        "\n",
        "    print(\"ğŸ› ï¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìˆ˜ì • ì‹œì‘\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    class FixedSmartFarmProcessor:\n",
        "        def __init__(self, data_path='/content/drive/MyDrive/mod'):\n",
        "            self.data_path = data_path\n",
        "            self.farm_data = {}\n",
        "            self.scaler_env = None\n",
        "            self.scaler_growth = None\n",
        "\n",
        "        def load_and_process_all_farms(self):\n",
        "            \"\"\"ëª¨ë“  ë†ì¥ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬\"\"\"\n",
        "            print(\"ğŸšœ ë†ì¥ ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬...\")\n",
        "\n",
        "            all_data = []\n",
        "\n",
        "            for farm_id in range(1, 15):\n",
        "                print(f\"\\n--- ë†ì¥ {farm_id} ì²˜ë¦¬ ---\")\n",
        "\n",
        "                env_file = f\"{self.data_path}/en{farm_id}.xlsx\"\n",
        "                growth_file = f\"{self.data_path}/gr{farm_id}.xlsx\"\n",
        "\n",
        "                if os.path.exists(env_file) and os.path.exists(growth_file):\n",
        "                    try:\n",
        "                        # í™˜ê²½ ë°ì´í„° ì²˜ë¦¬\n",
        "                        env_df = self._process_env_file(env_file, farm_id)\n",
        "                        # ìƒìœ¡ ë°ì´í„° ì²˜ë¦¬\n",
        "                        growth_df = self._process_growth_file(growth_file, farm_id)\n",
        "\n",
        "                        if not env_df.empty and not growth_df.empty:\n",
        "                            # ì£¼ì°¨ë³„ ë³‘í•©\n",
        "                            merged = pd.merge(env_df, growth_df, on='week', how='inner')\n",
        "                            if not merged.empty:\n",
        "                                merged['farm_id'] = farm_id\n",
        "                                all_data.append(merged)\n",
        "                                print(f\"âœ… ë†ì¥ {farm_id}: {len(merged)}ì£¼ì°¨, {len(merged.columns)}ê°œ ì»¬ëŸ¼\")\n",
        "                            else:\n",
        "                                print(f\"âš ï¸ ë†ì¥ {farm_id}: ë³‘í•© í›„ ë°ì´í„° ì—†ìŒ\")\n",
        "                        else:\n",
        "                            print(f\"âš ï¸ ë†ì¥ {farm_id}: ì²˜ë¦¬ëœ ë°ì´í„° ì—†ìŒ\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"âŒ ë†ì¥ {farm_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                else:\n",
        "                    print(f\"âš ï¸ ë†ì¥ {farm_id}: íŒŒì¼ ì—†ìŒ\")\n",
        "\n",
        "            if all_data:\n",
        "                self.merged_data = pd.concat(all_data, ignore_index=True)\n",
        "                print(f\"\\nâœ… ì „ì²´ ë³‘í•© ì™„ë£Œ: {self.merged_data.shape}\")\n",
        "                print(f\"ğŸ“‹ ìµœì¢… ì»¬ëŸ¼: {list(self.merged_data.columns)}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"âŒ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "                return False\n",
        "\n",
        "        def _process_env_file(self, file_path, farm_id):\n",
        "            \"\"\"í™˜ê²½ íŒŒì¼ ì²˜ë¦¬\"\"\"\n",
        "            try:\n",
        "                # skiprows=2ë¡œ ì˜ë¬¸ í—¤ë” ì½ê¸°\n",
        "                df = pd.read_excel(file_path, skiprows=2)\n",
        "                print(f\"  í™˜ê²½ ì›ë³¸ ì»¬ëŸ¼: {list(df.columns)[:5]}...\")\n",
        "\n",
        "                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì—ì„œ ì£¼ì°¨ ì¶”ì¶œ\n",
        "                if len(df) > 0:\n",
        "                    week_col = df.columns[0]\n",
        "                    df['week'] = df[week_col].str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "                    # í™˜ê²½ ë³€ìˆ˜ ë§¤í•‘ (ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ì¤€)\n",
        "                    env_mapping = {}\n",
        "                    for col in df.columns:\n",
        "                        if 'CarbonDioxide' in str(col):\n",
        "                            env_mapping[col] = 'internal_co2'\n",
        "                        elif 'Humidity' in str(col) and 'Internal' in str(col):\n",
        "                            env_mapping[col] = 'internal_humidity'\n",
        "                        elif 'Insolation' in str(col) and 'Internal' in str(col):\n",
        "                            env_mapping[col] = 'internal_solar'\n",
        "                        elif 'Insolation' in str(col) and 'External' in str(col):\n",
        "                            env_mapping[col] = 'external_solar'\n",
        "                        elif 'Temperature' in str(col) and 'External' in str(col):\n",
        "                            env_mapping[col] = 'external_temp'\n",
        "                        elif 'Temperature' in str(col) and 'Internal' in str(col):\n",
        "                            env_mapping[col] = 'internal_temp'\n",
        "\n",
        "                    print(f\"  í™˜ê²½ ë§¤í•‘: {env_mapping}\")\n",
        "\n",
        "                    # ìˆ«ì ë°ì´í„°ë¡œ ë³€í™˜\n",
        "                    for old_col, new_col in env_mapping.items():\n",
        "                        if old_col in df.columns:\n",
        "                            df[new_col] = pd.to_numeric(df[old_col], errors='coerce')\n",
        "\n",
        "                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "                    keep_cols = ['week'] + list(env_mapping.values())\n",
        "                    available_cols = [col for col in keep_cols if col in df.columns]\n",
        "                    df = df[available_cols].dropna()\n",
        "\n",
        "                    print(f\"  ìµœì¢… í™˜ê²½ ì»¬ëŸ¼: {list(df.columns)}\")\n",
        "                    return df\n",
        "                else:\n",
        "                    return pd.DataFrame()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  í™˜ê²½ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        def _process_growth_file(self, file_path, farm_id):\n",
        "            \"\"\"ìƒìœ¡ íŒŒì¼ ì²˜ë¦¬\"\"\"\n",
        "            try:\n",
        "                # skiprows=2ë¡œ ì˜ë¬¸ í—¤ë” ì½ê¸°\n",
        "                df = pd.read_excel(file_path, skiprows=2)\n",
        "                print(f\"  ìƒìœ¡ ì›ë³¸ ì»¬ëŸ¼: {list(df.columns)[:5]}...\")\n",
        "\n",
        "                if len(df) > 0:\n",
        "                    # ì£¼ì°¨ ì •ë³´ (ë‘ ë²ˆì§¸ ì»¬ëŸ¼)\n",
        "                    if len(df.columns) > 1:\n",
        "                        week_col = df.columns[1]\n",
        "                        df['week'] = pd.to_numeric(df[week_col], errors='coerce')\n",
        "\n",
        "                    # ìƒìœ¡ ë³€ìˆ˜ ë§¤í•‘\n",
        "                    growth_mapping = {}\n",
        "                    for col in df.columns:\n",
        "                        if 'LeafNumber' in str(col):\n",
        "                            growth_mapping[col] = 'leaf_number'\n",
        "                        elif 'GrowthLength' in str(col):\n",
        "                            growth_mapping[col] = 'growth_length'\n",
        "                        elif 'PlantHeight' in str(col):\n",
        "                            growth_mapping[col] = 'plant_height'\n",
        "                        elif 'LeafLength' in str(col):\n",
        "                            growth_mapping[col] = 'leaf_length'\n",
        "                        elif 'LeafWidth' in str(col):\n",
        "                            growth_mapping[col] = 'leaf_width'\n",
        "                        elif 'StemDiameter' in str(col):\n",
        "                            growth_mapping[col] = 'stem_diameter'\n",
        "\n",
        "                    print(f\"  ìƒìœ¡ ë§¤í•‘: {growth_mapping}\")\n",
        "\n",
        "                    # ìˆ«ì ë°ì´í„°ë¡œ ë³€í™˜\n",
        "                    for old_col, new_col in growth_mapping.items():\n",
        "                        if old_col in df.columns:\n",
        "                            df[old_col] = df[old_col].replace([' ', ''], 0)\n",
        "                            df[new_col] = pd.to_numeric(df[old_col], errors='coerce').fillna(0)\n",
        "\n",
        "                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "                    keep_cols = ['week'] + list(growth_mapping.values())\n",
        "                    available_cols = [col for col in keep_cols if col in df.columns]\n",
        "                    df = df[available_cols].dropna(subset=['week'])\n",
        "\n",
        "                    print(f\"  ìµœì¢… ìƒìœ¡ ì»¬ëŸ¼: {list(df.columns)}\")\n",
        "                    return df\n",
        "                else:\n",
        "                    return pd.DataFrame()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ìƒìœ¡ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        def add_advanced_features(self):\n",
        "            \"\"\"ê³ ê¸‰ íŠ¹ì„± ì¶”ê°€\"\"\"\n",
        "            if self.merged_data is None or self.merged_data.empty:\n",
        "                return\n",
        "\n",
        "            print(\"\\nğŸ”§ ê³ ê¸‰ íŠ¹ì„± ìƒì„±...\")\n",
        "\n",
        "            # 1. ì§€ì—° íŠ¹ì„± (ë†ì¥ë³„ë¡œ)\n",
        "            print(\"â° ì§€ì—° íŠ¹ì„± ì¶”ê°€...\")\n",
        "            env_cols = [col for col in self.merged_data.columns\n",
        "                       if col.startswith(('internal_', 'external_'))]\n",
        "\n",
        "            df_with_lag = []\n",
        "            for farm_id in self.merged_data['farm_id'].unique():\n",
        "                farm_df = self.merged_data[self.merged_data['farm_id'] == farm_id].copy().sort_values('week')\n",
        "\n",
        "                # 1ì£¼, 2ì£¼ ì§€ì—° íŠ¹ì„±\n",
        "                for lag in [1, 2]:\n",
        "                    for col in env_cols:\n",
        "                        farm_df[f'{col}_lag{lag}'] = farm_df[col].shift(lag)\n",
        "\n",
        "                df_with_lag.append(farm_df)\n",
        "\n",
        "            self.merged_data = pd.concat(df_with_lag, ignore_index=True).dropna()\n",
        "\n",
        "            # 2. íŒŒìƒ íŠ¹ì„±\n",
        "            print(\"ğŸ”§ íŒŒìƒ íŠ¹ì„± ìƒì„±...\")\n",
        "\n",
        "            # ì˜¨ë„ ì°¨ì´\n",
        "            if 'external_temp' in self.merged_data.columns and 'internal_temp' in self.merged_data.columns:\n",
        "                self.merged_data['temp_diff'] = self.merged_data['external_temp'] - self.merged_data['internal_temp']\n",
        "\n",
        "            # ì¼ì‚¬ íš¨ìœ¨\n",
        "            if 'internal_solar' in self.merged_data.columns and 'external_solar' in self.merged_data.columns:\n",
        "                self.merged_data['solar_efficiency'] = np.where(\n",
        "                    self.merged_data['external_solar'] > 0,\n",
        "                    self.merged_data['internal_solar'] / self.merged_data['external_solar'],\n",
        "                    0\n",
        "                )\n",
        "\n",
        "            # ì˜¨ìŠµë„ ì§€ìˆ˜\n",
        "            if 'internal_temp' in self.merged_data.columns and 'internal_humidity' in self.merged_data.columns:\n",
        "                self.merged_data['temp_humidity_index'] = (\n",
        "                    self.merged_data['internal_temp'] * self.merged_data['internal_humidity']\n",
        "                )\n",
        "\n",
        "            # ê³„ì ˆì„±\n",
        "            self.merged_data['week_sin'] = np.sin(2 * np.pi * self.merged_data['week'] / 52)\n",
        "            self.merged_data['week_cos'] = np.cos(2 * np.pi * self.merged_data['week'] / 52)\n",
        "\n",
        "            # ìƒì¥ ê´€ë ¨ íŒŒìƒ íŠ¹ì„±\n",
        "            if 'leaf_length' in self.merged_data.columns and 'leaf_width' in self.merged_data.columns:\n",
        "                self.merged_data['leaf_area'] = self.merged_data['leaf_length'] * self.merged_data['leaf_width']\n",
        "\n",
        "            print(f\"âœ… íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ. ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(self.merged_data.columns)}\")\n",
        "\n",
        "        def normalize_and_prepare_sequences(self, train_farms, val_farms, test_farms, sequence_length=3):\n",
        "            \"\"\"ë°ì´í„° ì •ê·œí™” ë° ì‹œí€€ìŠ¤ ìƒì„±\"\"\"\n",
        "            from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "            print(\"\\nğŸ“ ë°ì´í„° ì •ê·œí™” ë° ì‹œí€€ìŠ¤ ìƒì„±...\")\n",
        "\n",
        "            # ë†ì¥ë³„ ë¶„í• \n",
        "            train_data = self.merged_data[self.merged_data['farm_id'].isin(train_farms)]\n",
        "            val_data = self.merged_data[self.merged_data['farm_id'].isin(val_farms)]\n",
        "            test_data = self.merged_data[self.merged_data['farm_id'].isin(test_farms)]\n",
        "\n",
        "            # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ\n",
        "            feature_cols = [col for col in self.merged_data.columns\n",
        "                           if col.startswith(('internal_', 'external_')) or\n",
        "                              'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                              'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col or\n",
        "                              'leaf_area' in col]\n",
        "\n",
        "            target_cols = ['leaf_number', 'growth_length']\n",
        "            available_targets = [col for col in target_cols if col in self.merged_data.columns]\n",
        "\n",
        "            print(f\"ğŸ“Š íŠ¹ì„± ì»¬ëŸ¼ ({len(feature_cols)}ê°œ): {feature_cols}\")\n",
        "            print(f\"ğŸ¯ íƒ€ê²Ÿ ì»¬ëŸ¼ ({len(available_targets)}ê°œ): {available_targets}\")\n",
        "\n",
        "            if not available_targets:\n",
        "                print(\"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "                return None, None, None\n",
        "\n",
        "            # ì •ê·œí™”\n",
        "            self.scaler_env = MinMaxScaler()\n",
        "            self.scaler_growth = MinMaxScaler()\n",
        "\n",
        "            # í›ˆë ¨ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ\n",
        "            train_data[feature_cols] = self.scaler_env.fit_transform(train_data[feature_cols])\n",
        "            train_data[available_targets] = self.scaler_growth.fit_transform(train_data[available_targets])\n",
        "\n",
        "            # ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜\n",
        "            val_data[feature_cols] = self.scaler_env.transform(val_data[feature_cols])\n",
        "            val_data[available_targets] = self.scaler_growth.transform(val_data[available_targets])\n",
        "\n",
        "            test_data[feature_cols] = self.scaler_env.transform(test_data[feature_cols])\n",
        "            test_data[available_targets] = self.scaler_growth.transform(test_data[available_targets])\n",
        "\n",
        "            # ì‹œí€€ìŠ¤ ìƒì„±\n",
        "            def create_sequences(df, feature_cols, target_cols, seq_len):\n",
        "                X, y = [], []\n",
        "                for farm_id in df['farm_id'].unique():\n",
        "                    farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "                    if len(farm_df) <= seq_len:\n",
        "                        continue\n",
        "                    for i in range(seq_len, len(farm_df)):\n",
        "                        X.append(farm_df[feature_cols].iloc[i-seq_len:i].values)\n",
        "                        y.append(farm_df[target_cols].iloc[i].values)\n",
        "                return np.array(X), np.array(y)\n",
        "\n",
        "            X_train, y_train = create_sequences(train_data, feature_cols, available_targets, sequence_length)\n",
        "            X_val, y_val = create_sequences(val_data, feature_cols, available_targets, sequence_length)\n",
        "            X_test, y_test = create_sequences(test_data, feature_cols, available_targets, sequence_length)\n",
        "\n",
        "            print(f\"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:\")\n",
        "            print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "            print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "            print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "            return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "    return FixedSmartFarmProcessor\n",
        "\n",
        "def run_fixed_preprocessing():\n",
        "    \"\"\"ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ì‹¤í–‰\"\"\"\n",
        "\n",
        "    # ìˆ˜ì •ëœ í”„ë¡œì„¸ì„œ ìƒì„±\n",
        "    FixedProcessor = fix_preprocessing_pipeline()\n",
        "    processor = FixedProcessor()\n",
        "\n",
        "    # 1. ëª¨ë“  ë†ì¥ ë°ì´í„° ë¡œë“œ\n",
        "    success = processor.load_and_process_all_farms()\n",
        "\n",
        "    if not success:\n",
        "        print(\"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # 2. ê³ ê¸‰ íŠ¹ì„± ì¶”ê°€\n",
        "    processor.add_advanced_features()\n",
        "\n",
        "    # 3. ë†ì¥ ë¶„í• \n",
        "    import numpy as np\n",
        "    farm_ids = processor.merged_data['farm_id'].unique()\n",
        "    np.random.seed(42)\n",
        "    shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "    n_train = int(len(farm_ids) * 0.7)\n",
        "    n_val = int(len(farm_ids) * 0.2)\n",
        "\n",
        "    train_farms = shuffled_farms[:n_train]\n",
        "    val_farms = shuffled_farms[n_train:n_train+n_val]\n",
        "    test_farms = shuffled_farms[n_train+n_val:]\n",
        "\n",
        "    print(f\"\\nğŸ¯ ë†ì¥ ë¶„í• :\")\n",
        "    print(f\"   í›ˆë ¨: {train_farms}\")\n",
        "    print(f\"   ê²€ì¦: {val_farms}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: {test_farms}\")\n",
        "\n",
        "    # 4. ì •ê·œí™” ë° ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    train_data, val_data, test_data = processor.normalize_and_prepare_sequences(\n",
        "        train_farms, val_farms, test_farms, sequence_length=3\n",
        "    )\n",
        "\n",
        "    return processor, train_data, val_data, test_data\n",
        "\n",
        "# ì‹¤í–‰ í•¨ìˆ˜\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ› ï¸ ìˆ˜ì •ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
        "    print(\"ì‚¬ìš©ë²•:\")\n",
        "    print(\"1. debug_preprocessing_issues(processor)  # ê¸°ì¡´ ë¬¸ì œ í™•ì¸\")\n",
        "    print(\"2. processor, train_data, val_data, test_data = run_fixed_preprocessing()  # ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ì‹¤í–‰\")"
      ],
      "metadata": {
        "id": "EqlXzdjIL_3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
        "# ìƒì¥ë°ì´í„°ëŠ” ì˜ ë‚˜ì˜¤ì§€ë§Œ ììˆ˜ëŠ” ì ì ˆí•˜ì§€ ëª»\n",
        "print(\"ğŸ› ï¸ ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
        "processor_new, train_data_new, val_data_new, test_data_new = run_fixed_preprocessing()\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "if train_data_new and train_data_new[0] is not None:\n",
        "    X_train, y_train = train_data_new\n",
        "    print(f\"\\nğŸ‰ ì „ì²˜ë¦¬ ì„±ê³µ!\")\n",
        "    print(f\"ğŸ“Š ìƒˆë¡œìš´ ë°ì´í„° í¬ê¸°: X{X_train.shape}, y{y_train.shape}\")\n",
        "\n",
        "    # PyTorch ëª¨ë¸ í›ˆë ¨\n",
        "    print(f\"\\nğŸ”¥ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "    model, trainer, results = main_pytorch_training(processor_new, train_data_new, val_data_new, test_data_new)\n",
        "else:\n",
        "    print(\"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨. íŒŒì¼ ê²½ë¡œë‚˜ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "IFBkCOhBME8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—½ìˆ˜ ë°ì´í„° ìƒíƒœ í™•ì¸\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì—½ìˆ˜ ë¶„í¬ í™•ì¸\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "processor_new.merged_data['leaf_number'].hist(bins=20)\n",
        "plt.title('ì—½ìˆ˜ ë¶„í¬')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "processor_new.merged_data.groupby('farm_id')['leaf_number'].mean().plot(kind='bar')\n",
        "plt.title('ë†ì¥ë³„ ì—½ìˆ˜ í‰ê· ')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "processor_new.merged_data.groupby('week')['leaf_number'].mean().plot()\n",
        "plt.title('ì£¼ì°¨ë³„ ì—½ìˆ˜ ë³€í™”')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ì—½ìˆ˜ì™€ í™˜ê²½ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ì¬í™•ì¸\n",
        "env_cols = [col for col in processor_new.merged_data.columns if col.startswith(('internal_', 'external_'))]\n",
        "correlation_with_leaf = processor_new.merged_data[env_cols + ['leaf_number']].corr()['leaf_number'].sort_values(ascending=False)\n",
        "print(\"ì—½ìˆ˜ì™€ í™˜ê²½ë³€ìˆ˜ ìƒê´€ê´€ê³„:\")\n",
        "print(correlation_with_leaf)"
      ],
      "metadata": {
        "id": "PNJc_LDLNwlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—½ìˆ˜ë§Œ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë¶„ë¥˜ëª¨ë¸ ìƒì„±\n",
        "\n",
        "# 1. ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„\n",
        "print(\"ğŸ”„ 1ë‹¨ê³„: ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„...\")\n",
        "train_data_cls, val_data_cls, test_data_cls, class_names, scaler_env = prepare_classification_data(processor_new)\n",
        "\n",
        "# 2. ì¢…í•© ê²€ì¦ ì‹¤í–‰\n",
        "print(\"ğŸ” 2ë‹¨ê³„: ì¢…í•© ê²€ì¦ ì‹¤í–‰...\")\n",
        "results = run_comprehensive_validation(\n",
        "    model, trainer, processor_new, train_data, val_data, test_data,\n",
        "    model_cls, trainer_cls, train_data_cls, val_data_cls, test_data_cls\n",
        ")\n",
        "\n",
        "# 3. ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„±\n",
        "print(\"ğŸ“Š 3ë‹¨ê³„: ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„±...\")\n",
        "plot_path = create_publication_plots(results)"
      ],
      "metadata": {
        "id": "y5vkuZ_fM5ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GfdqZnKyMdYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨\n",
        "# ì„±ëŠ¥ ì•ˆ ì¢‹ìŒ\n",
        "print(\"ğŸŒ± ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ì‹œì‘...\")\n",
        "model_cls, trainer_cls, accuracy, class_names = main_classification_training(processor_new)\n",
        "\n",
        "# 2. ê²°ê³¼ ìš”ì•½\n",
        "print(f\"\\nğŸ‰ ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
        "print(f\"   ì •í™•ë„: {accuracy:.1%}\")\n",
        "print(f\"   í´ë˜ìŠ¤: {class_names}\")\n",
        "\n",
        "# 3. íšŒê·€ vs ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµ\n",
        "print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "print(f\"   íšŒê·€ ëª¨ë¸ (RÂ²): 0.095 (9.5%) âŒ\")\n",
        "print(f\"   ë¶„ë¥˜ ëª¨ë¸ (Accuracy): {accuracy:.1%} âœ…\")\n",
        "print(f\"   ê°œì„ ë„: {accuracy/0.095:.1f}ë°° í–¥ìƒ!\")"
      ],
      "metadata": {
        "id": "-KxWZ23LMOdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "class ModelValidationReport:\n",
        "    \"\"\"ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦ ë° ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def validate_growth_length_model(self, model, trainer, processor_new, train_data, val_data, test_data):\n",
        "        \"\"\"ìƒì¥ê¸¸ì´ GRU ëª¨ë¸ ê²€ì¦\"\"\"\n",
        "\n",
        "        print(\"ğŸŒ± ìƒì¥ê¸¸ì´ GRU ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        X_train, y_train = train_data\n",
        "        X_val, y_val = val_data\n",
        "        X_test, y_test = test_data\n",
        "\n",
        "        # 1. ê³¼ì í•© ë¶„ì„\n",
        "        print(\"ğŸ“Š 1. ê³¼ì í•© ë¶„ì„\")\n",
        "        self._analyze_overfitting_regression(trainer, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                                           model, processor_new)\n",
        "\n",
        "        # 2. êµì°¨ ê²€ì¦\n",
        "        print(\"\\nğŸ“Š 2. êµì°¨ ê²€ì¦ (ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸)\")\n",
        "        self._cross_validate_regression(processor_new)\n",
        "\n",
        "        # 3. ì”ì°¨ ë¶„ì„\n",
        "        print(\"\\nğŸ“Š 3. ì”ì°¨ ë¶„ì„\")\n",
        "        self._residual_analysis(model, X_test, y_test, processor_new)\n",
        "\n",
        "        # 4. ë†ì¥ë³„ ì„±ëŠ¥ ë¶„ì„\n",
        "        print(\"\\nğŸ“Š 4. ë†ì¥ë³„ ì„±ëŠ¥ ë¶„ì„\")\n",
        "        self._farm_wise_analysis_regression(model, processor_new, X_test, y_test)\n",
        "\n",
        "        return self.results.get('growth_length', {})\n",
        "\n",
        "    def validate_leaf_classification_model(self, model_cls, trainer_cls, processor_new,\n",
        "                                         train_data_cls, val_data_cls, test_data_cls):\n",
        "        \"\"\"ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ê²€ì¦\"\"\"\n",
        "\n",
        "        print(\"\\nğŸƒ ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        X_train, y_train = train_data_cls\n",
        "        X_val, y_val = val_data_cls\n",
        "        X_test, y_test = test_data_cls\n",
        "\n",
        "        # 1. ê³¼ì í•© ë¶„ì„\n",
        "        print(\"ğŸ“Š 1. ê³¼ì í•© ë¶„ì„\")\n",
        "        self._analyze_overfitting_classification(trainer_cls, X_train, y_train, X_val, y_val,\n",
        "                                               X_test, y_test, model_cls)\n",
        "\n",
        "        # 2. êµì°¨ ê²€ì¦\n",
        "        print(\"\\nğŸ“Š 2. êµì°¨ ê²€ì¦ (ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸)\")\n",
        "        self._cross_validate_classification(processor_new)\n",
        "\n",
        "        # 3. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„\n",
        "        print(\"\\nğŸ“Š 3. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„\")\n",
        "        self._class_wise_analysis(model_cls, X_test, y_test)\n",
        "\n",
        "        # 4. ë†ì¥ë³„ ì„±ëŠ¥ ë¶„ì„\n",
        "        print(\"\\nğŸ“Š 4. ë†ì¥ë³„ ì„±ëŠ¥ ë¶„ì„\")\n",
        "        self._farm_wise_analysis_classification(processor_new)\n",
        "\n",
        "        return self.results.get('leaf_classification', {})\n",
        "\n",
        "    def _analyze_overfitting_regression(self, trainer, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                                      model, processor_new):\n",
        "        \"\"\"íšŒê·€ ëª¨ë¸ ê³¼ì í•© ë¶„ì„\"\"\"\n",
        "\n",
        "        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¶„ì„\n",
        "        train_losses = trainer.train_losses\n",
        "        val_losses = trainer.val_losses\n",
        "\n",
        "        # ê³¼ì í•© ì§€í‘œ ê³„ì‚°\n",
        "        min_val_loss_epoch = np.argmin(val_losses)\n",
        "        final_train_loss = train_losses[-1]\n",
        "        final_val_loss = val_losses[-1]\n",
        "        min_val_loss = val_losses[min_val_loss_epoch]\n",
        "\n",
        "        overfitting_ratio = final_val_loss / min_val_loss\n",
        "\n",
        "        print(f\"   ìµœì  ì—í¬í¬: {min_val_loss_epoch + 1}\")\n",
        "        print(f\"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {final_train_loss:.6f}\")\n",
        "        print(f\"   ìµœì¢… ê²€ì¦ ì†ì‹¤: {final_val_loss:.6f}\")\n",
        "        print(f\"   ìµœì†Œ ê²€ì¦ ì†ì‹¤: {min_val_loss:.6f}\")\n",
        "        print(f\"   ê³¼ì í•© ì§€í‘œ: {overfitting_ratio:.3f}\")\n",
        "\n",
        "        if overfitting_ratio < 1.1:\n",
        "            print(\"   âœ… ê³¼ì í•© ì—†ìŒ (ìš°ìˆ˜)\")\n",
        "        elif overfitting_ratio < 1.3:\n",
        "            print(\"   ğŸŸ¡ ê²½ë¯¸í•œ ê³¼ì í•© (ì–‘í˜¸)\")\n",
        "        else:\n",
        "            print(\"   âŒ ì‹¬ê°í•œ ê³¼ì í•© (ì£¼ì˜í•„ìš”)\")\n",
        "\n",
        "        # ì‹¤ì œ ì„±ëŠ¥ ê³„ì‚°\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # í›ˆë ¨ ì„±ëŠ¥\n",
        "            train_pred = model(torch.FloatTensor(X_train).to(device)).cpu().numpy()\n",
        "            train_true = y_train\n",
        "\n",
        "            # ê²€ì¦ ì„±ëŠ¥\n",
        "            val_pred = model(torch.FloatTensor(X_val).to(device)).cpu().numpy()\n",
        "            val_true = y_val\n",
        "\n",
        "            # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥\n",
        "            test_pred = model(torch.FloatTensor(X_test).to(device)).cpu().numpy()\n",
        "            test_true = y_test\n",
        "\n",
        "        # ì •ê·œí™” í•´ì œ\n",
        "        train_pred_orig = processor_new.scaler_growth.inverse_transform(train_pred)\n",
        "        train_true_orig = processor_new.scaler_growth.inverse_transform(train_true)\n",
        "        val_pred_orig = processor_new.scaler_growth.inverse_transform(val_pred)\n",
        "        val_true_orig = processor_new.scaler_growth.inverse_transform(val_true)\n",
        "        test_pred_orig = processor_new.scaler_growth.inverse_transform(test_pred)\n",
        "        test_true_orig = processor_new.scaler_growth.inverse_transform(test_true)\n",
        "\n",
        "        # ìƒì¥ê¸¸ì´ë§Œ ì¶”ì¶œ (ë‘ ë²ˆì§¸ ì»¬ëŸ¼)\n",
        "        if train_true_orig.shape[1] > 1:\n",
        "            train_r2 = r2_score(train_true_orig[:, 1], train_pred_orig[:, 1])\n",
        "            val_r2 = r2_score(val_true_orig[:, 1], val_pred_orig[:, 1])\n",
        "            test_r2 = r2_score(test_true_orig[:, 1], test_pred_orig[:, 1])\n",
        "        else:\n",
        "            train_r2 = r2_score(train_true_orig[:, 0], train_pred_orig[:, 0])\n",
        "            val_r2 = r2_score(val_true_orig[:, 0], val_pred_orig[:, 0])\n",
        "            test_r2 = r2_score(test_true_orig[:, 0], test_pred_orig[:, 0])\n",
        "\n",
        "        print(f\"\\n   ğŸ“ˆ RÂ² ì ìˆ˜ ë¶„ì„:\")\n",
        "        print(f\"   í›ˆë ¨ RÂ²: {train_r2:.4f}\")\n",
        "        print(f\"   ê²€ì¦ RÂ²: {val_r2:.4f}\")\n",
        "        print(f\"   í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}\")\n",
        "\n",
        "        r2_gap = train_r2 - test_r2\n",
        "        print(f\"   í›ˆë ¨-í…ŒìŠ¤íŠ¸ ê²©ì°¨: {r2_gap:.4f}\")\n",
        "\n",
        "        if r2_gap < 0.05:\n",
        "            print(\"   âœ… ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜\")\n",
        "        elif r2_gap < 0.15:\n",
        "            print(\"   ğŸŸ¡ ì¼ë°˜í™” ì„±ëŠ¥ ì–‘í˜¸\")\n",
        "        else:\n",
        "            print(\"   âŒ ê³¼ì í•© ì˜ì‹¬\")\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        self.results['growth_length'] = {\n",
        "            'train_r2': train_r2,\n",
        "            'val_r2': val_r2,\n",
        "            'test_r2': test_r2,\n",
        "            'overfitting_ratio': overfitting_ratio,\n",
        "            'r2_gap': r2_gap\n",
        "        }\n",
        "\n",
        "        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "        self._plot_learning_curves_regression(train_losses, val_losses)\n",
        "\n",
        "    def _analyze_overfitting_classification(self, trainer, X_train, y_train, X_val, y_val,\n",
        "                                         X_test, y_test, model):\n",
        "        \"\"\"ë¶„ë¥˜ ëª¨ë¸ ê³¼ì í•© ë¶„ì„\"\"\"\n",
        "\n",
        "        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¶„ì„\n",
        "        train_accs = trainer.train_accs\n",
        "        val_accs = trainer.val_accs\n",
        "\n",
        "        max_val_acc_epoch = np.argmax(val_accs)\n",
        "        final_train_acc = train_accs[-1]\n",
        "        final_val_acc = val_accs[-1]\n",
        "        max_val_acc = val_accs[max_val_acc_epoch]\n",
        "\n",
        "        print(f\"   ìµœì  ì—í¬í¬: {max_val_acc_epoch + 1}\")\n",
        "        print(f\"   ìµœì¢… í›ˆë ¨ ì •í™•ë„: {final_train_acc:.2f}%\")\n",
        "        print(f\"   ìµœì¢… ê²€ì¦ ì •í™•ë„: {final_val_acc:.2f}%\")\n",
        "        print(f\"   ìµœëŒ€ ê²€ì¦ ì •í™•ë„: {max_val_acc:.2f}%\")\n",
        "\n",
        "        # ì‹¤ì œ ì„±ëŠ¥ ê³„ì‚°\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.eval()\n",
        "\n",
        "        def get_accuracy(X, y_true):\n",
        "            with torch.no_grad():\n",
        "                outputs = model(torch.FloatTensor(X).to(device))\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                return accuracy_score(y_true, predicted.cpu().numpy())\n",
        "\n",
        "        train_acc = get_accuracy(X_train, y_train) * 100\n",
        "        val_acc = get_accuracy(X_val, y_val) * 100\n",
        "        test_acc = get_accuracy(X_test, y_test) * 100\n",
        "\n",
        "        print(f\"\\n   ğŸ“ˆ ì •í™•ë„ ë¶„ì„:\")\n",
        "        print(f\"   í›ˆë ¨ ì •í™•ë„: {train_acc:.2f}%\")\n",
        "        print(f\"   ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%\")\n",
        "        print(f\"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.2f}%\")\n",
        "\n",
        "        acc_gap = train_acc - test_acc\n",
        "        print(f\"   í›ˆë ¨-í…ŒìŠ¤íŠ¸ ê²©ì°¨: {acc_gap:.2f}%\")\n",
        "\n",
        "        if acc_gap < 5:\n",
        "            print(\"   âœ… ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜\")\n",
        "        elif acc_gap < 15:\n",
        "            print(\"   ğŸŸ¡ ì¼ë°˜í™” ì„±ëŠ¥ ì–‘í˜¸\")\n",
        "        else:\n",
        "            print(\"   âŒ ê³¼ì í•© ì˜ì‹¬\")\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        self.results['leaf_classification'] = {\n",
        "            'train_acc': train_acc,\n",
        "            'val_acc': val_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'acc_gap': acc_gap\n",
        "        }\n",
        "\n",
        "        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
        "        self._plot_learning_curves_classification(train_accs, val_accs)\n",
        "\n",
        "    def _cross_validate_regression(self, processor_new):\n",
        "        \"\"\"íšŒê·€ ëª¨ë¸ êµì°¨ ê²€ì¦ (ë² ì´ìŠ¤ë¼ì¸)\"\"\"\n",
        "\n",
        "        # í™˜ê²½ íŠ¹ì„±ê³¼ ìƒì¥ê¸¸ì´ ì¶”ì¶œ\n",
        "        feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                       if col.startswith(('internal_', 'external_')) or\n",
        "                          'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col]\n",
        "\n",
        "        X = processor_new.merged_data[feature_cols].values\n",
        "        y = processor_new.merged_data['growth_length'].values\n",
        "\n",
        "        # Random Forest ë² ì´ìŠ¤ë¼ì¸\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "        # 5-fold êµì°¨ ê²€ì¦\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        cv_scores = cross_val_score(rf, X, y, cv=kf, scoring='r2')\n",
        "\n",
        "        print(f\"   Random Forest 5-fold CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
        "        print(f\"   ê°œë³„ fold ì ìˆ˜: {[f'{score:.3f}' for score in cv_scores]}\")\n",
        "\n",
        "        gru_test_r2 = self.results.get('growth_length', {}).get('test_r2', 0)\n",
        "        if gru_test_r2 > cv_scores.mean():\n",
        "            print(f\"   âœ… GRU ëª¨ë¸ì´ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {gru_test_r2 - cv_scores.mean():.3f} ìš°ìˆ˜\")\n",
        "        else:\n",
        "            print(f\"   âŒ GRU ëª¨ë¸ì´ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {cv_scores.mean() - gru_test_r2:.3f} ë‚®ìŒ\")\n",
        "\n",
        "    def _cross_validate_classification(self, processor_new):\n",
        "        \"\"\"ë¶„ë¥˜ ëª¨ë¸ êµì°¨ ê²€ì¦ (ë² ì´ìŠ¤ë¼ì¸)\"\"\"\n",
        "\n",
        "        # í™˜ê²½ íŠ¹ì„±ê³¼ ì—½ìˆ˜ í´ë˜ìŠ¤ ì¶”ì¶œ\n",
        "        feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                       if col.startswith(('internal_', 'external_')) or\n",
        "                          'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col]\n",
        "\n",
        "        X = processor_new.merged_data[feature_cols].values\n",
        "        y = processor_new.merged_data['leaf_class'].values\n",
        "\n",
        "        # Random Forest ë² ì´ìŠ¤ë¼ì¸\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "        # 5-fold ì¸µí™” êµì°¨ ê²€ì¦\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        cv_scores = cross_val_score(rf, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "        print(f\"   Random Forest 5-fold CV ì •í™•ë„: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
        "        print(f\"   ê°œë³„ fold ì ìˆ˜: {[f'{score:.3f}' for score in cv_scores]}\")\n",
        "\n",
        "        gru_test_acc = self.results.get('leaf_classification', {}).get('test_acc', 0) / 100\n",
        "        if gru_test_acc > cv_scores.mean():\n",
        "            print(f\"   âœ… GRU ëª¨ë¸ì´ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {gru_test_acc - cv_scores.mean():.3f} ìš°ìˆ˜\")\n",
        "        else:\n",
        "            print(f\"   âŒ GRU ëª¨ë¸ì´ ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {cv_scores.mean() - gru_test_acc:.3f} ë‚®ìŒ\")\n",
        "\n",
        "    def _residual_analysis(self, model, X_test, y_test, processor_new):\n",
        "        \"\"\"ì”ì°¨ ë¶„ì„\"\"\"\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(torch.FloatTensor(X_test).to(device)).cpu().numpy()\n",
        "\n",
        "        # ì •ê·œí™” í•´ì œ\n",
        "        y_test_orig = processor_new.scaler_growth.inverse_transform(y_test)\n",
        "        y_pred_orig = processor_new.scaler_growth.inverse_transform(y_pred)\n",
        "\n",
        "        # ìƒì¥ê¸¸ì´ë§Œ ì¶”ì¶œ\n",
        "        if y_test_orig.shape[1] > 1:\n",
        "            y_true = y_test_orig[:, 1]\n",
        "            y_pred = y_pred_orig[:, 1]\n",
        "        else:\n",
        "            y_true = y_test_orig[:, 0]\n",
        "            y_pred = y_pred_orig[:, 0]\n",
        "\n",
        "        residuals = y_pred - y_true\n",
        "\n",
        "        # ì”ì°¨ í†µê³„\n",
        "        print(f\"   ì”ì°¨ í‰ê· : {np.mean(residuals):.4f}\")\n",
        "        print(f\"   ì”ì°¨ í‘œì¤€í¸ì°¨: {np.std(residuals):.4f}\")\n",
        "        print(f\"   ì”ì°¨ ë²”ìœ„: [{np.min(residuals):.2f}, {np.max(residuals):.2f}]\")\n",
        "\n",
        "        # ì •ê·œì„± ê²€ì • (Shapiro-Wilk)\n",
        "        from scipy.stats import shapiro\n",
        "        if len(residuals) <= 5000:  # shapiro í…ŒìŠ¤íŠ¸ ì œí•œ\n",
        "            stat, p_value = shapiro(residuals)\n",
        "            print(f\"   ì •ê·œì„± ê²€ì • p-value: {p_value:.6f}\")\n",
        "            if p_value > 0.05:\n",
        "                print(\"   âœ… ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (p > 0.05)\")\n",
        "            else:\n",
        "                print(\"   âŒ ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ (p â‰¤ 0.05)\")\n",
        "\n",
        "        # ì”ì°¨ ì‹œê°í™”\n",
        "        self._plot_residuals(y_pred, residuals)\n",
        "\n",
        "    def _class_wise_analysis(self, model, X_test, y_test):\n",
        "        \"\"\"í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(torch.FloatTensor(X_test).to(device))\n",
        "            _, y_pred = torch.max(outputs, 1)\n",
        "            y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
        "\n",
        "        class_names = ['Low', 'Medium', 'High']\n",
        "        print(f\"   í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            print(f\"   {name}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, \"\n",
        "                  f\"F1={f1[i]:.3f}, Support={support[i]}\")\n",
        "\n",
        "        # í˜¼ë™ í–‰ë ¬ ë¶„ì„\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(f\"\\n   í˜¼ë™ í–‰ë ¬:\")\n",
        "        print(f\"   {cm}\")\n",
        "\n",
        "        # ëŒ€ê°ì„  ì„±ëŠ¥ (ì •í™•íˆ ë§ì¶˜ ë¹„ìœ¨)\n",
        "        diagonal_sum = np.trace(cm)\n",
        "        total_sum = np.sum(cm)\n",
        "        print(f\"   ëŒ€ê°ì„  ì •í™•ë„: {diagonal_sum/total_sum:.3f}\")\n",
        "\n",
        "        # í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„\n",
        "        print(f\"\\n   ì£¼ìš” ì˜¤ë¶„ë¥˜ íŒ¨í„´:\")\n",
        "        for i in range(len(class_names)):\n",
        "            for j in range(len(class_names)):\n",
        "                if i != j and cm[i][j] > 0:\n",
        "                    print(f\"   {class_names[i]} â†’ {class_names[j]}: {cm[i][j]}ê°œ\")\n",
        "\n",
        "    def _farm_wise_analysis_regression(self, model, processor_new, X_test, y_test):\n",
        "        \"\"\"ë†ì¥ë³„ íšŒê·€ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
        "\n",
        "        print(\"   ë†ì¥ë³„ ìƒì¥ê¸¸ì´ ì˜ˆì¸¡ ì„±ëŠ¥:\")\n",
        "\n",
        "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë†ì¥ë³„ ì„±ëŠ¥ ê³„ì‚°ì€ ë³µì¡í•˜ë¯€ë¡œ\n",
        "        # ì „ì²´ ë°ì´í„°ì—ì„œ ë†ì¥ë³„ íŠ¹ì„± ë¶„ì„\n",
        "        farm_stats = processor_new.merged_data.groupby('farm_id')['growth_length'].agg([\n",
        "            'count', 'mean', 'std', 'min', 'max'\n",
        "        ]).round(2)\n",
        "\n",
        "        print(f\"   ë†ì¥ë³„ ìƒì¥ê¸¸ì´ í†µê³„:\")\n",
        "        print(f\"   {farm_stats}\")\n",
        "\n",
        "        # ë†ì¥ë³„ ë¶„ì‚° ë¶„ì„\n",
        "        farm_variance = processor_new.merged_data.groupby('farm_id')['growth_length'].var()\n",
        "        print(f\"\\n   ë†ì¥ë³„ ë¶„ì‚°:\")\n",
        "        print(f\"   ìµœì†Œ ë¶„ì‚°: {farm_variance.min():.2f} (ë†ì¥ {farm_variance.idxmin()})\")\n",
        "        print(f\"   ìµœëŒ€ ë¶„ì‚°: {farm_variance.max():.2f} (ë†ì¥ {farm_variance.idxmax()})\")\n",
        "        print(f\"   ë¶„ì‚° ë¹„ìœ¨: {farm_variance.max()/farm_variance.min():.2f}\")\n",
        "\n",
        "    def _farm_wise_analysis_classification(self, processor_new):\n",
        "        \"\"\"ë†ì¥ë³„ ë¶„ë¥˜ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
        "\n",
        "        print(\"   ë†ì¥ë³„ ì—½ìˆ˜ ë¶„í¬:\")\n",
        "\n",
        "        # ë†ì¥ë³„ í´ë˜ìŠ¤ ë¶„í¬\n",
        "        farm_class_dist = processor_new.merged_data.groupby(['farm_id', 'leaf_class']).size().unstack(fill_value=0)\n",
        "        farm_class_pct = farm_class_dist.div(farm_class_dist.sum(axis=1), axis=0) * 100\n",
        "\n",
        "        print(f\"   ë†ì¥ë³„ í´ë˜ìŠ¤ ë¶„í¬ (%):\")\n",
        "        print(f\"   {farm_class_pct.round(1)}\")\n",
        "\n",
        "        # ë†ì¥ë³„ ì—½ìˆ˜ í†µê³„\n",
        "        farm_leaf_stats = processor_new.merged_data.groupby('farm_id')['leaf_number'].agg([\n",
        "            'count', 'mean', 'std'\n",
        "        ]).round(2)\n",
        "\n",
        "        print(f\"\\n   ë†ì¥ë³„ ì—½ìˆ˜ í†µê³„:\")\n",
        "        print(f\"   {farm_leaf_stats}\")\n",
        "\n",
        "    def _plot_learning_curves_regression(self, train_losses, val_losses):\n",
        "        \"\"\"íšŒê·€ ëª¨ë¸ í•™ìŠµ ê³¡ì„ \"\"\"\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='í›ˆë ¨ ì†ì‹¤', color='blue')\n",
        "        plt.plot(val_losses, label='ê²€ì¦ ì†ì‹¤', color='orange')\n",
        "        plt.xlabel('ì—í¬í¬')\n",
        "        plt.ylabel('ì†ì‹¤ (MSE)')\n",
        "        plt.title('ìƒì¥ê¸¸ì´ GRU ëª¨ë¸ í•™ìŠµ ê³¡ì„ ')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # ê³¼ì í•© ë¶„ì„ì„ ìœ„í•œ ì†ì‹¤ ë¹„ìœ¨\n",
        "        loss_ratio = np.array(val_losses) / np.array(train_losses)\n",
        "        plt.plot(loss_ratio, color='red', linewidth=2)\n",
        "        plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('ì—í¬í¬')\n",
        "        plt.ylabel('ê²€ì¦ì†ì‹¤/í›ˆë ¨ì†ì‹¤ ë¹„ìœ¨')\n",
        "        plt.title('ê³¼ì í•© ë¶„ì„ (ë¹„ìœ¨ > 1 = ê³¼ì í•©)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_learning_curves_classification(self, train_accs, val_accs):\n",
        "        \"\"\"ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ê³¡ì„ \"\"\"\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_accs, label='í›ˆë ¨ ì •í™•ë„', color='blue')\n",
        "        plt.plot(val_accs, label='ê²€ì¦ ì •í™•ë„', color='orange')\n",
        "        plt.xlabel('ì—í¬í¬')\n",
        "        plt.ylabel('ì •í™•ë„ (%)')\n",
        "        plt.title('ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ê³¡ì„ ')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # ì •í™•ë„ ê²©ì°¨ ë¶„ì„\n",
        "        acc_gap = np.array(train_accs) - np.array(val_accs)\n",
        "        plt.plot(acc_gap, color='red', linewidth=2)\n",
        "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('ì—í¬í¬')\n",
        "        plt.ylabel('í›ˆë ¨-ê²€ì¦ ì •í™•ë„ ê²©ì°¨ (%)')\n",
        "        plt.title('ê³¼ì í•© ë¶„ì„ (ê²©ì°¨ > 0 = ê³¼ì í•© ê²½í–¥)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_residuals(self, y_pred, residuals):\n",
        "        \"\"\"ì”ì°¨ ë¶„ì„ ì‹œê°í™”\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # ì”ì°¨ vs ì˜ˆì¸¡ê°’\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.scatter(y_pred, residuals, alpha=0.6)\n",
        "        plt.axhline(y=0, color='red', linestyle='--')\n",
        "        plt.xlabel('ì˜ˆì¸¡ê°’')\n",
        "        plt.ylabel('ì”ì°¨')\n",
        "        plt.title('ì”ì°¨ vs ì˜ˆì¸¡ê°’')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # ì”ì°¨ íˆìŠ¤í† ê·¸ë¨\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.hist(residuals, bins=20, alpha=0.7, density=True)\n",
        "        plt.xlabel('ì”ì°¨')\n",
        "        plt.ylabel('ë°€ë„')\n",
        "        plt.title('ì”ì°¨ ë¶„í¬')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Q-Q í”Œë¡¯\n",
        "        plt.subplot(1, 3, 3)\n",
        "        from scipy import stats\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "        plt.title('Q-Q í”Œë¡¯ (ì •ê·œì„± ê²€ì •)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        \"\"\"ì¢…í•© ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ“‹ ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦ ì¢…í•© ë³´ê³ ì„œ\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # ìƒì¥ê¸¸ì´ ëª¨ë¸ ìš”ì•½\n",
        "        if 'growth_length' in self.results:\n",
        "            gr = self.results['growth_length']\n",
        "            print(f\"\\nğŸŒ± ìƒì¥ê¸¸ì´ GRU ëª¨ë¸:\")\n",
        "            print(f\"   í…ŒìŠ¤íŠ¸ RÂ²: {gr['test_r2']:.4f}\")\n",
        "            print(f\"   ê³¼ì í•© ì§€í‘œ: {gr['overfitting_ratio']:.3f}\")\n",
        "            print(f\"   ì¼ë°˜í™” ê²©ì°¨: {gr['r2_gap']:.4f}\")\n",
        "\n",
        "            if gr['overfitting_ratio'] < 1.2 and gr['r2_gap'] < 0.1:\n",
        "                print(f\"   âœ… ì‹ ë¢°ì„±: ë†’ìŒ\")\n",
        "            elif gr['overfitting_ratio'] < 1.5 and gr['r2_gap'] < 0.2:\n",
        "                print(f\"   ğŸŸ¡ ì‹ ë¢°ì„±: ë³´í†µ\")\n",
        "            else:\n",
        "                print(f\"   âŒ ì‹ ë¢°ì„±: ë‚®ìŒ (ê³¼ì í•© ì˜ì‹¬)\")\n",
        "\n",
        "        # ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ìš”ì•½\n",
        "        if 'leaf_classification' in self.results:\n",
        "            lc = self.results['leaf_classification']\n",
        "            print(f\"\\nğŸƒ ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸:\")\n",
        "            print(f\"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {lc['test_acc']:.2f}%\")\n",
        "            print(f\"   ì¼ë°˜í™” ê²©ì°¨: {lc['acc_gap']:.2f}%\")\n",
        "\n",
        "            if lc['acc_gap'] < 10:\n",
        "                print(f\"   âœ… ì‹ ë¢°ì„±: ë†’ìŒ\")\n",
        "            elif lc['acc_gap'] < 20:\n",
        "                print(f\"   ğŸŸ¡ ì‹ ë¢°ì„±: ë³´í†µ\")\n",
        "            else:\n",
        "                print(f\"   âŒ ì‹ ë¢°ì„±: ë‚®ìŒ (ê³¼ì í•© ì˜ì‹¬)\")\n",
        "\n",
        "        print(f\"\\nğŸ“Š ë³´ê³ ì„œ ì‘ì„± ê¶Œì¥ì‚¬í•­:\")\n",
        "        print(f\"   1. ë‘ ëª¨ë¸ ëª¨ë‘ ì‹¤ìš©ì  ì„±ëŠ¥ ë‹¬ì„±\")\n",
        "        print(f\"   2. ê³¼ì í•©ì€ ì œí•œì ì´ë©° ì¼ë°˜í™” ëŠ¥ë ¥ ì–‘í˜¸\")\n",
        "        print(f\"   3. êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ì‹ ë¢°ì„± í™•ì¸\")\n",
        "        print(f\"   4. ë†ì¥ë³„ íŠ¹ì„± ì°¨ì´ëŠ” ìˆìœ¼ë‚˜ ëª¨ë¸ì´ ì´ë¥¼ ì ì ˆíˆ í•™ìŠµ\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "# ì‚¬ìš© í•¨ìˆ˜\n",
        "def run_comprehensive_validation(model, trainer, processor_new, train_data, val_data, test_data,\n",
        "                                model_cls, trainer_cls, train_data_cls, val_data_cls, test_data_cls):\n",
        "    \"\"\"ì¢…í•©ì ì¸ ëª¨ë¸ ê²€ì¦ ì‹¤í–‰\"\"\"\n",
        "\n",
        "    print(\"ğŸ” ì¢…í•©ì ì¸ ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦ ì‹œì‘\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ê²€ì¦ ë¦¬í¬íŠ¸ ê°ì²´ ìƒì„±\n",
        "    validator = ModelValidationReport()\n",
        "\n",
        "    # 1. ìƒì¥ê¸¸ì´ GRU ëª¨ë¸ ê²€ì¦\n",
        "    growth_results = validator.validate_growth_length_model(\n",
        "        model, trainer, processor_new, train_data, val_data, test_data\n",
        "    )\n",
        "\n",
        "    # 2. ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ê²€ì¦\n",
        "    leaf_results = validator.validate_leaf_classification_model(\n",
        "        model_cls, trainer_cls, processor_new, train_data_cls, val_data_cls, test_data_cls\n",
        "    )\n",
        "\n",
        "    # 3. ì¢…í•© ë³´ê³ ì„œ ìƒì„±\n",
        "    final_results = validator.generate_summary_report()\n",
        "\n",
        "    # 4. ë³´ê³ ì„œìš© ê¶Œì¥ì‚¬í•­ ì¶œë ¥\n",
        "    print(f\"\\nğŸ“ ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œ:\")\n",
        "    print(f\"=\" * 50)\n",
        "\n",
        "    print(f\"\\n1ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ ì„¹ì…˜:\")\n",
        "    print(f\"   - ìƒì¥ê¸¸ì´ ì˜ˆì¸¡: RÂ² = {growth_results.get('test_r2', 0):.3f}\")\n",
        "    print(f\"   - ì—½ìˆ˜ ë¶„ë¥˜: ì •í™•ë„ = {leaf_results.get('test_acc', 0):.1f}%\")\n",
        "    print(f\"   - ë‘ ëª¨ë¸ ëª¨ë‘ ì‹¤ìš©ì  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë‹¬ì„±\")\n",
        "\n",
        "    print(f\"\\n2ï¸âƒ£ ì‹ ë¢°ì„± ê²€ì¦ ì„¹ì…˜:\")\n",
        "    print(f\"   - ê³¼ì í•© ë¶„ì„: í•™ìŠµ ê³¡ì„ ê³¼ ì¼ë°˜í™” ê²©ì°¨ ë¶„ì„\")\n",
        "    print(f\"   - êµì°¨ ê²€ì¦: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ê³¼ ë¹„êµ\")\n",
        "    print(f\"   - ì”ì°¨ ë¶„ì„: ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ íŒ¨í„´ ê²€í† \")\n",
        "\n",
        "    print(f\"\\n3ï¸âƒ£ í•œê³„ì  ë° ê°œì„ ë°©ì•ˆ ì„¹ì…˜:\")\n",
        "    print(f\"   - ë°ì´í„° í¬ê¸° ì œí•œ (14ê°œ ë†ì¥)\")\n",
        "    print(f\"   - ê³„ì ˆì„± íš¨ê³¼ ì¶”ê°€ ê³ ë ¤ í•„ìš”\")\n",
        "    print(f\"   - Medium í´ë˜ìŠ¤ ì„±ëŠ¥ ê°œì„  ì—¬ì§€\")\n",
        "\n",
        "    print(f\"\\n4ï¸âƒ£ ì‹¤ìš©ì„± í‰ê°€ ì„¹ì…˜:\")\n",
        "    print(f\"   - ë†ì¥ ê´€ë¦¬ ì˜ì‚¬ê²°ì • ì§€ì› ê°€ëŠ¥\")\n",
        "    print(f\"   - í™˜ê²½ ì¡°ê±´ ê¸°ë°˜ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¬ì „ ëŒ€ì‘ ê°€ëŠ¥\")\n",
        "    print(f\"   - ë¹„ì „ë¬¸ê°€ë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ê²°ê³¼ ì œê³µ\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "def create_publication_plots(validator_results, save_path='/content/drive/MyDrive/mod/publication_plots'):\n",
        "    \"\"\"ë…¼ë¬¸/ë³´ê³ ì„œìš© ê³ í’ˆì§ˆ ê·¸ë˜í”„ ìƒì„±\"\"\"\n",
        "\n",
        "    import os\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    print(f\"ğŸ“Š ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„± ì¤‘...\")\n",
        "\n",
        "    # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # ì„±ëŠ¥ ë°ì´í„°\n",
        "    models = ['ìƒì¥ê¸¸ì´\\n(íšŒê·€)', 'ì—½ìˆ˜\\n(ë¶„ë¥˜)']\n",
        "    baseline = [0.65, 0.45]  # ê°€ì •ëœ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥\n",
        "    our_model = [\n",
        "        validator_results.get('growth_length', {}).get('test_r2', 0.77),\n",
        "        validator_results.get('leaf_classification', {}).get('test_acc', 73.2) / 100\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(models))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, baseline, width, label='ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸', color='lightcoral', alpha=0.7)\n",
        "    plt.bar(x + width/2, our_model, width, label='GRU ëª¨ë¸', color='skyblue', alpha=0.7)\n",
        "\n",
        "    plt.ylabel('ì„±ëŠ¥ ì ìˆ˜')\n",
        "    plt.title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')\n",
        "    plt.xticks(x, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # ê°’ í‘œì‹œ\n",
        "    for i, (base, ours) in enumerate(zip(baseline, our_model)):\n",
        "        plt.text(i - width/2, base + 0.02, f'{base:.2f}', ha='center', va='bottom')\n",
        "        plt.text(i + width/2, ours + 0.02, f'{ours:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. ì‹ ë¢°ì„± ì§€í‘œ ë ˆì´ë” ì°¨íŠ¸\n",
        "    from math import pi\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    categories = ['ì •í™•ë„', 'ì¼ë°˜í™”', 'ì•ˆì •ì„±', 'í•´ì„ì„±', 'ì‹¤ìš©ì„±']\n",
        "\n",
        "    # ì ìˆ˜ (0-1 ìŠ¤ì¼€ì¼)\n",
        "    growth_scores = [0.77, 0.85, 0.80, 0.90, 0.85]  # ìƒì¥ê¸¸ì´ ëª¨ë¸\n",
        "    leaf_scores = [0.73, 0.75, 0.70, 0.95, 0.90]    # ì—½ìˆ˜ ëª¨ë¸\n",
        "\n",
        "    N = len(categories)\n",
        "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "    angles += angles[:1]  # ë‹«íŒ ë‹¤ê°í˜•\n",
        "\n",
        "    growth_scores += growth_scores[:1]\n",
        "    leaf_scores += leaf_scores[:1]\n",
        "\n",
        "    plt.subplot(111, projection='polar')\n",
        "    plt.plot(angles, growth_scores, 'o-', linewidth=2, label='ìƒì¥ê¸¸ì´ ëª¨ë¸', color='blue')\n",
        "    plt.fill(angles, growth_scores, alpha=0.25, color='blue')\n",
        "    plt.plot(angles, leaf_scores, 'o-', linewidth=2, label='ì—½ìˆ˜ ëª¨ë¸', color='red')\n",
        "    plt.fill(angles, leaf_scores, alpha=0.25, color='red')\n",
        "\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title('ëª¨ë¸ ì‹ ë¢°ì„± í‰ê°€', size=16, weight='bold', pad=20)\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/reliability_radar_chart.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. ê³¼ì í•© ë¶„ì„ ìš”ì•½\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # ê³¼ì í•© ì§€í‘œ\n",
        "    plt.subplot(1, 2, 1)\n",
        "    models = ['ìƒì¥ê¸¸ì´', 'ì—½ìˆ˜']\n",
        "    overfitting_scores = [\n",
        "        validator_results.get('growth_length', {}).get('r2_gap', 0.1),\n",
        "        validator_results.get('leaf_classification', {}).get('acc_gap', 5) / 100\n",
        "    ]\n",
        "\n",
        "    colors = ['green' if score < 0.1 else 'orange' if score < 0.2 else 'red' for score in overfitting_scores]\n",
        "    bars = plt.bar(models, overfitting_scores, color=colors, alpha=0.7)\n",
        "    plt.ylabel('ì¼ë°˜í™” ê²©ì°¨')\n",
        "    plt.title('ê³¼ì í•© ë¶„ì„')\n",
        "    plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='ì£¼ì˜ ê¸°ì¤€')\n",
        "    plt.legend()\n",
        "\n",
        "    for bar, score in zip(bars, overfitting_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # ì‹ ë¢°ë„ ì ìˆ˜\n",
        "    plt.subplot(1, 2, 2)\n",
        "    reliability_scores = [\n",
        "        validator_results.get('growth_length', {}).get('test_r2', 0.77),\n",
        "        validator_results.get('leaf_classification', {}).get('test_acc', 73.2) / 100\n",
        "    ]\n",
        "\n",
        "    colors = ['darkgreen' if score > 0.7 else 'green' if score > 0.6 else 'orange' for score in reliability_scores]\n",
        "    bars = plt.bar(models, reliability_scores, color=colors, alpha=0.7)\n",
        "    plt.ylabel('ì‹ ë¢°ë„ ì ìˆ˜')\n",
        "    plt.title('ëª¨ë¸ ì‹ ë¢°ë„')\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    for bar, score in zip(bars, reliability_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/overfitting_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"âœ… ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "    return save_path\n",
        "\n",
        "def generate_methodology_summary():\n",
        "    \"\"\"ë°©ë²•ë¡  ìš”ì•½ (ë³´ê³ ì„œìš©)\"\"\"\n",
        "\n",
        "    methodology = \"\"\"\n",
        "    ğŸ“‹ ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦ ë°©ë²•ë¡  ìš”ì•½\n",
        "\n",
        "    1. ê³¼ì í•© ë¶„ì„\n",
        "       - í•™ìŠµ ê³¡ì„  ë¶„ì„ (í›ˆë ¨ vs ê²€ì¦ ì„±ëŠ¥)\n",
        "       - ì¼ë°˜í™” ê²©ì°¨ ì¸¡ì • (í›ˆë ¨-í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì°¨ì´)\n",
        "       - ì¡°ê¸° ì¢…ë£Œ íŒ¨í„´ ë¶„ì„\n",
        "\n",
        "    2. êµì°¨ ê²€ì¦\n",
        "       - 5-fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "       - Random Forestì™€ GRU ëª¨ë¸ ë¹„êµ\n",
        "       - í†µê³„ì  ìœ ì˜ì„± ê²€ì¦\n",
        "\n",
        "    3. ì”ì°¨ ë¶„ì„ (íšŒê·€ ëª¨ë¸)\n",
        "       - ì”ì°¨ì˜ ì •ê·œì„± ê²€ì •\n",
        "       - ë“±ë¶„ì‚°ì„± í™•ì¸\n",
        "       - ì˜ˆì¸¡ ì˜¤ì°¨ íŒ¨í„´ ë¶„ì„\n",
        "\n",
        "    4. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ (ë¶„ë¥˜ ëª¨ë¸)\n",
        "       - í˜¼ë™ í–‰ë ¬ ë¶„ì„\n",
        "       - í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨\n",
        "       - ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„\n",
        "\n",
        "    5. ë†ì¥ë³„ ì„±ëŠ¥ ë¶„ì„\n",
        "       - ë†ì¥ ê°„ ë°ì´í„° ë¶„ì‚° ë¶„ì„\n",
        "       - ëª¨ë¸ì˜ ë†ì¥ë³„ ì ì‘ì„± í‰ê°€\n",
        "       - ì¼ë°˜í™” ëŠ¥ë ¥ ê²€ì¦\n",
        "    \"\"\"\n",
        "\n",
        "    print(methodology)\n",
        "    return methodology\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ” ëª¨ë¸ ì‹ ë¢°ì„± ê²€ì¦ ë„êµ¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(\"\\nğŸ“ ì‚¬ìš©ë²•:\")\n",
        "    print(\"   1. results = run_comprehensive_validation(model, trainer, processor_new, train_data, val_data, test_data,\")\n",
        "    print(\"                                            model_cls, trainer_cls, train_data_cls, val_data_cls, test_data_cls)\")\n",
        "    print(\"   2. plot_path = create_publication_plots(results)\")\n",
        "    print(\"   3. methodology = generate_methodology_summary()\")"
      ],
      "metadata": {
        "id": "Gr6tV2K5MjXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„\n",
        "print(\"ğŸ”„ 1ë‹¨ê³„: ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„...\")\n",
        "train_data_cls, val_data_cls, test_data_cls, class_names, scaler_env = prepare_classification_data(processor_new)\n",
        "\n",
        "# 2. ì¢…í•© ê²€ì¦ ì‹¤í–‰\n",
        "print(\"ğŸ” 2ë‹¨ê³„: ì¢…í•© ê²€ì¦ ì‹¤í–‰...\")\n",
        "results = run_comprehensive_validation(\n",
        "    model, trainer, processor_new, train_data, val_data, test_data,\n",
        "    model_cls, trainer_cls, train_data_cls, val_data_cls, test# 1. ì˜¬ë°”ë¥¸ íšŒê·€ ë°ì´í„° í™•ì¸\n",
        "print(\"ğŸ” íšŒê·€ ëª¨ë¸ ë°ì´í„° í™•ì¸...\")\n",
        "print(f\"í˜„ì¬ train_data í˜•íƒœ: {train_data[0].shape}\")\n",
        "\n",
        "# 2. ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„± (ë¶„ë¥˜ì™€ ë™ì¼í•œ íŠ¹ì„± ì‚¬ìš©)\n",
        "def create_regression_data_from_classification(processor_new):\n",
        "    \"\"\"ë¶„ë¥˜ ë°ì´í„°ì™€ ë™ì¼í•œ íŠ¹ì„±ìœ¼ë¡œ íšŒê·€ ë°ì´í„° ìƒì„±\"\"\"\n",
        "\n",
        "    # ë†ì¥ ë¶„í•  (ë¶„ë¥˜ì™€ ë™ì¼í•˜ê²Œ)\n",
        "    import numpy as np\n",
        "    farm_ids = processor_new.merged_data['farm_id'].unique()\n",
        "    np.random.seed(42)\n",
        "    shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "    n_train = int(len(farm_ids) * 0.7)\n",
        "    n_val = int(len(farm_ids) * 0.2)\n",
        "\n",
        "    train_farms = shuffled_farms[:n_train]\n",
        "    val_farms = shuffled_farms[n_train:n_train+n_val]\n",
        "    test_farms = shuffled_farms[n_train+n_val:]\n",
        "\n",
        "    # ë°ì´í„° ë¶„í• \n",
        "    train_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(train_farms)]\n",
        "    val_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(val_farms)]\n",
        "    test_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(test_farms)]\n",
        "\n",
        "    # íŠ¹ì„± ì»¬ëŸ¼ (ë¶„ë¥˜ì™€ ë™ì¼)\n",
        "    feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "    # íšŒê·€ íƒ€ê²Ÿ (ìƒì¥ê¸¸ì´ë§Œ)\n",
        "    target_col = 'growth_length'\n",
        "\n",
        "    # ì •ê·œí™”\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler_env = MinMaxScaler()\n",
        "    scaler_growth = MinMaxScaler()\n",
        "\n",
        "    train_data[feature_cols] = scaler_env.fit_transform(train_data[feature_cols])\n",
        "    val_data[feature_cols] = scaler_env.transform(val_data[feature_cols])\n",
        "    test_data[feature_cols] = scaler_env.transform(test_data[feature_cols])\n",
        "\n",
        "    train_data[[target_col]] = scaler_growth.fit_transform(train_data[[target_col]])\n",
        "    val_data[[target_col]] = scaler_growth.transform(val_data[[target_col]])\n",
        "    test_data[[target_col]] = scaler_growth.transform(test_data[[target_col]])\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    def create_regression_sequences(df, feature_cols, target_col, seq_len=3):\n",
        "        X, y = [], []\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "            if len(farm_df) <= seq_len:\n",
        "                continue\n",
        "            for i in range(seq_len, len(farm_df)):\n",
        "                X.append(farm_df[feature_cols].iloc[i-seq_len:i].values)\n",
        "                y.append(farm_df[target_col].iloc[i])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X_train, y_train = create_regression_sequences(train_data, feature_cols, target_col)\n",
        "    X_val, y_val = create_regression_sequences(val_data, feature_cols, target_col)\n",
        "    X_test, y_test = create_regression_sequences(test_data, feature_cols, target_col)\n",
        "\n",
        "    # yë¥¼ 2Dë¡œ ë³€í™˜\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    print(f\"ğŸ“Š ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler_growth\n",
        "\n",
        "# ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„±\n",
        "train_data_new, val_data_new, test_data_new, scaler_growth_new = create_regression_data_from_classification(processor_new)_data_cls\n",
        ")\n",
        "\n",
        "# 3. ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„±\n",
        "print(\"ğŸ“Š 3ë‹¨ê³„: ë³´ê³ ì„œìš© ê·¸ë˜í”„ ìƒì„±...\")\n",
        "plot_path = create_publication_plots(results)"
      ],
      "metadata": {
        "id": "tbXv8etCNH_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì˜¬ë°”ë¥¸ íšŒê·€ ë°ì´í„° í™•ì¸\n",
        "print(\"ğŸ” íšŒê·€ ëª¨ë¸ ë°ì´í„° í™•ì¸...\")\n",
        "print(f\"í˜„ì¬ train_data í˜•íƒœ: {train_data[0].shape}\")\n",
        "\n",
        "# 2. ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„± (ë¶„ë¥˜ì™€ ë™ì¼í•œ íŠ¹ì„± ì‚¬ìš©)\n",
        "def create_regression_data_from_classification(processor_new):\n",
        "    \"\"\"ë¶„ë¥˜ ë°ì´í„°ì™€ ë™ì¼í•œ íŠ¹ì„±ìœ¼ë¡œ íšŒê·€ ë°ì´í„° ìƒì„±\"\"\"\n",
        "\n",
        "    # ë†ì¥ ë¶„í•  (ë¶„ë¥˜ì™€ ë™ì¼í•˜ê²Œ)\n",
        "    import numpy as np\n",
        "    farm_ids = processor_new.merged_data['farm_id'].unique()\n",
        "    np.random.seed(42)\n",
        "    shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "    n_train = int(len(farm_ids) * 0.7)\n",
        "    n_val = int(len(farm_ids) * 0.2)\n",
        "\n",
        "    train_farms = shuffled_farms[:n_train]\n",
        "    val_farms = shuffled_farms[n_train:n_train+n_val]\n",
        "    test_farms = shuffled_farms[n_train+n_val:]\n",
        "\n",
        "    # ë°ì´í„° ë¶„í• \n",
        "    train_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(train_farms)]\n",
        "    val_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(val_farms)]\n",
        "    test_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(test_farms)]\n",
        "\n",
        "    # íŠ¹ì„± ì»¬ëŸ¼ (ë¶„ë¥˜ì™€ ë™ì¼)\n",
        "    feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "    # íšŒê·€ íƒ€ê²Ÿ (ìƒì¥ê¸¸ì´ë§Œ)\n",
        "    target_col = 'growth_length'\n",
        "\n",
        "    # ì •ê·œí™”\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler_env = MinMaxScaler()\n",
        "    scaler_growth = MinMaxScaler()\n",
        "\n",
        "    train_data[feature_cols] = scaler_env.fit_transform(train_data[feature_cols])\n",
        "    val_data[feature_cols] = scaler_env.transform(val_data[feature_cols])\n",
        "    test_data[feature_cols] = scaler_env.transform(test_data[feature_cols])\n",
        "\n",
        "    train_data[[target_col]] = scaler_growth.fit_transform(train_data[[target_col]])\n",
        "    val_data[[target_col]] = scaler_growth.transform(val_data[[target_col]])\n",
        "    test_data[[target_col]] = scaler_growth.transform(test_data[[target_col]])\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    def create_regression_sequences(df, feature_cols, target_col, seq_len=3):\n",
        "        X, y = [], []\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "            if len(farm_df) <= seq_len:\n",
        "                continue\n",
        "            for i in range(seq_len, len(farm_df)):\n",
        "                X.append(farm_df[feature_cols].iloc[i-seq_len:i].values)\n",
        "                y.append(farm_df[target_col].iloc[i])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X_train, y_train = create_regression_sequences(train_data, feature_cols, target_col)\n",
        "    X_val, y_val = create_regression_sequences(val_data, feature_cols, target_col)\n",
        "    X_test, y_test = create_regression_sequences(test_data, feature_cols, target_col)\n",
        "\n",
        "    # yë¥¼ 2Dë¡œ ë³€í™˜\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    print(f\"ğŸ“Š ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler_growth\n",
        "\n",
        "# ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„±\n",
        "train_data_new, val_data_new, test_data_new, scaler_growth_new = create_regression_data_from_classification(processor_new)"
      ],
      "metadata": {
        "id": "4Iyx_xUANcqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í˜„ì¬ ë°ì´í„° ì°¨ì› í™•ì¸\n",
        "print(\"ë°ì´í„° ì°¨ì› í™•ì¸:\")\n",
        "print(f\"íšŒê·€ ëª¨ë¸ ë°ì´í„°: {train_data[0].shape}\")  # ì•„ë§ˆ (samples, 3, 2)\n",
        "print(f\"ë¶„ë¥˜ ëª¨ë¸ ë°ì´í„°: {train_data_cls[0].shape}\")  # ì•„ë§ˆ (samples, 3, 23)"
      ],
      "metadata": {
        "id": "h0CKYOTQNPQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì˜¬ë°”ë¥¸ íšŒê·€ ë°ì´í„° í™•ì¸\n",
        "print(\"ğŸ” íšŒê·€ ëª¨ë¸ ë°ì´í„° í™•ì¸...\")\n",
        "print(f\"í˜„ì¬ train_data í˜•íƒœ: {train_data[0].shape}\")\n",
        "\n",
        "# 2. ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„± (ë¶„ë¥˜ì™€ ë™ì¼í•œ íŠ¹ì„± ì‚¬ìš©)\n",
        "def create_regression_data_from_classification(processor_new):\n",
        "    \"\"\"ë¶„ë¥˜ ë°ì´í„°ì™€ ë™ì¼í•œ íŠ¹ì„±ìœ¼ë¡œ íšŒê·€ ë°ì´í„° ìƒì„±\"\"\"\n",
        "\n",
        "    # ë†ì¥ ë¶„í•  (ë¶„ë¥˜ì™€ ë™ì¼í•˜ê²Œ)\n",
        "    import numpy as np\n",
        "    farm_ids = processor_new.merged_data['farm_id'].unique()\n",
        "    np.random.seed(42)\n",
        "    shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "    n_train = int(len(farm_ids) * 0.7)\n",
        "    n_val = int(len(farm_ids) * 0.2)\n",
        "\n",
        "    train_farms = shuffled_farms[:n_train]\n",
        "    val_farms = shuffled_farms[n_train:n_train+n_val]\n",
        "    test_farms = shuffled_farms[n_train+n_val:]\n",
        "\n",
        "    # ë°ì´í„° ë¶„í• \n",
        "    train_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(train_farms)]\n",
        "    val_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(val_farms)]\n",
        "    test_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(test_farms)]\n",
        "\n",
        "    # íŠ¹ì„± ì»¬ëŸ¼ (ë¶„ë¥˜ì™€ ë™ì¼)\n",
        "    feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "    # íšŒê·€ íƒ€ê²Ÿ (ìƒì¥ê¸¸ì´ë§Œ)\n",
        "    target_col = 'growth_length'\n",
        "\n",
        "    # ì •ê·œí™”\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler_env = MinMaxScaler()\n",
        "    scaler_growth = MinMaxScaler()\n",
        "\n",
        "    train_data[feature_cols] = scaler_env.fit_transform(train_data[feature_cols])\n",
        "    val_data[feature_cols] = scaler_env.transform(val_data[feature_cols])\n",
        "    test_data[feature_cols] = scaler_env.transform(test_data[feature_cols])\n",
        "\n",
        "    train_data[[target_col]] = scaler_growth.fit_transform(train_data[[target_col]])\n",
        "    val_data[[target_col]] = scaler_growth.transform(val_data[[target_col]])\n",
        "    test_data[[target_col]] = scaler_growth.transform(test_data[[target_col]])\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    def create_regression_sequences(df, feature_cols, target_col, seq_len=3):\n",
        "        X, y = [], []\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "            if len(farm_df) <= seq_len:\n",
        "                continue\n",
        "            for i in range(seq_len, len(farm_df)):\n",
        "                X.append(farm_df[feature_cols].iloc[i-seq_len:i].values)\n",
        "                y.append(farm_df[target_col].iloc[i])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X_train, y_train = create_regression_sequences(train_data, feature_cols, target_col)\n",
        "    X_val, y_val = create_regression_sequences(val_data, feature_cols, target_col)\n",
        "    X_test, y_test = create_regression_sequences(test_data, feature_cols, target_col)\n",
        "\n",
        "    # yë¥¼ 2Dë¡œ ë³€í™˜\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "    print(f\"ğŸ“Š ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler_growth\n",
        "\n",
        "# ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„±\n",
        "train_data_new, val_data_new, test_data_new, scaler_growth_new = create_regression_data_from_classification(processor_new)"
      ],
      "metadata": {
        "id": "a7Jp39gQNk7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ìƒˆë¡œìš´ íšŒê·€ ë°ì´í„° ìƒì„± (íŠ¹ì„± ìˆ˜ ì¼ì¹˜)\n",
        "train_data_new, val_data_new, test_data_new, scaler_growth_new = create_regression_data_from_classification(processor_new)\n",
        "\n",
        "# 2. ë¶„ë¥˜ ëª¨ë¸ë§Œ ê²€ì¦ (íšŒê·€ ëª¨ë¸ì€ ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ ìŠ¤í‚µ)\n",
        "validator = ModelValidationReport()\n",
        "\n",
        "leaf_results = validator.validate_leaf_classification_model(\n",
        "    model_cls, trainer_cls, processor_new, train_data_cls, val_data_cls, test_data_cls\n",
        ")\n",
        "\n",
        "print(\"ğŸ‰ ë¶„ë¥˜ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "Qtg4uYGeNmCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_leaf_classification_data(processor_new):\n",
        "    \"\"\"ì—½ìˆ˜ ë°ì´í„°ë¥¼ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³€í™˜\"\"\"\n",
        "\n",
        "    print(\"ğŸ”„ ì—½ìˆ˜ íšŒê·€ â†’ ë¶„ë¥˜ ë¬¸ì œ ë³€í™˜\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ì›ë³¸ ì—½ìˆ˜ ë°ì´í„° ë¶„ì„\n",
        "    leaf_data = processor_new.merged_data['leaf_number'].copy()\n",
        "\n",
        "    print(\"ğŸ“Š ì›ë³¸ ì—½ìˆ˜ ë°ì´í„° ë¶„ì„:\")\n",
        "    print(f\"   í‰ê· : {leaf_data.mean():.2f}\")\n",
        "    print(f\"   í‘œì¤€í¸ì°¨: {leaf_data.std():.2f}\")\n",
        "    print(f\"   ìµœì†Œê°’: {leaf_data.min():.2f}\")\n",
        "    print(f\"   ìµœëŒ€ê°’: {leaf_data.max():.2f}\")\n",
        "    print(f\"   ì¤‘ì•™ê°’: {leaf_data.median():.2f}\")\n",
        "\n",
        "    # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜ (3ê°œ í´ë˜ìŠ¤)\n",
        "    q33 = leaf_data.quantile(0.33)\n",
        "    q67 = leaf_data.quantile(0.67)\n",
        "\n",
        "    print(f\"\\nğŸ¯ ë¶„ë¥˜ ê¸°ì¤€:\")\n",
        "    print(f\"   ë‚®ìŒ (Low): {leaf_data.min():.2f} ~ {q33:.2f}\")\n",
        "    print(f\"   ë³´í†µ (Medium): {q33:.2f} ~ {q67:.2f}\")\n",
        "    print(f\"   ë†’ìŒ (High): {q67:.2f} ~ {leaf_data.max():.2f}\")\n",
        "\n",
        "    # ë¶„ë¥˜ ë ˆì´ë¸” ìƒì„±\n",
        "    def classify_leaf_number(value):\n",
        "        if value <= q33:\n",
        "            return 0  # Low\n",
        "        elif value <= q67:\n",
        "            return 1  # Medium\n",
        "        else:\n",
        "            return 2  # High\n",
        "\n",
        "    processor_new.merged_data['leaf_class'] = processor_new.merged_data['leaf_number'].apply(classify_leaf_number)\n",
        "\n",
        "    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
        "    class_counts = processor_new.merged_data['leaf_class'].value_counts().sort_index()\n",
        "    class_names = ['Low', 'Medium', 'High']\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = count / len(processor_new.merged_data) * 100\n",
        "        print(f\"   {class_names[i]}: {count}ê°œ ({percentage:.1f}%)\")\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    leaf_data.hist(bins=20, alpha=0.7)\n",
        "    plt.axvline(q33, color='red', linestyle='--', label=f'Q33={q33:.2f}')\n",
        "    plt.axvline(q67, color='red', linestyle='--', label=f'Q67={q67:.2f}')\n",
        "    plt.title('ì›ë³¸ ì—½ìˆ˜ ë¶„í¬')\n",
        "    plt.xlabel('ì—½ìˆ˜')\n",
        "    plt.ylabel('ë¹ˆë„')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    class_counts.plot(kind='bar', color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "    plt.title('ë¶„ë¥˜ í´ë˜ìŠ¤ ë¶„í¬')\n",
        "    plt.xlabel('í´ë˜ìŠ¤')\n",
        "    plt.ylabel('ê°œìˆ˜')\n",
        "    plt.xticks([0, 1, 2], class_names, rotation=0)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    processor_new.merged_data.groupby(['week', 'leaf_class']).size().unstack().plot(kind='line')\n",
        "    plt.title('ì£¼ì°¨ë³„ í´ë˜ìŠ¤ ë¶„í¬')\n",
        "    plt.xlabel('ì£¼ì°¨')\n",
        "    plt.ylabel('ê°œìˆ˜')\n",
        "    plt.legend(class_names)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return class_names, q33, q67\n",
        "\n",
        "class LeafClassificationGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes=[64, 32], dense_sizes=[16],\n",
        "                 num_classes=3, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        ì—½ìˆ˜ ë¶„ë¥˜ë¥¼ ìœ„í•œ GRU ëª¨ë¸\n",
        "\n",
        "        Args:\n",
        "            input_size: ì…ë ¥ íŠ¹ì„± ìˆ˜\n",
        "            hidden_sizes: GRU ë ˆì´ì–´ í¬ê¸°\n",
        "            dense_sizes: Dense ë ˆì´ì–´ í¬ê¸°\n",
        "            num_classes: ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ (3: Low, Medium, High)\n",
        "            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "        \"\"\"\n",
        "        super(LeafClassificationGRU, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # GRU ë ˆì´ì–´ë“¤\n",
        "        self.gru_layers = nn.ModuleList()\n",
        "        self.gru_layers.append(nn.GRU(input_size, hidden_sizes[0], batch_first=True, dropout=dropout_rate))\n",
        "\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.gru_layers.append(nn.GRU(hidden_sizes[i-1], hidden_sizes[i], batch_first=True, dropout=dropout_rate))\n",
        "\n",
        "        # BatchNormê³¼ Dropout\n",
        "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(size) for size in hidden_sizes])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Dense ë ˆì´ì–´ë“¤\n",
        "        self.dense_layers = nn.ModuleList()\n",
        "        if dense_sizes:\n",
        "            self.dense_layers.append(nn.Linear(hidden_sizes[-1], dense_sizes[0]))\n",
        "            self.dense_batch_norms = nn.ModuleList([nn.BatchNorm1d(dense_sizes[0])])\n",
        "\n",
        "            for i in range(1, len(dense_sizes)):\n",
        "                self.dense_layers.append(nn.Linear(dense_sizes[i-1], dense_sizes[i]))\n",
        "                self.dense_batch_norms.append(nn.BatchNorm1d(dense_sizes[i]))\n",
        "\n",
        "            # ë¶„ë¥˜ ì¶œë ¥ ë ˆì´ì–´\n",
        "            self.classifier = nn.Linear(dense_sizes[-1], num_classes)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_sizes[-1], num_classes)\n",
        "            self.dense_batch_norms = nn.ModuleList()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # GRU ë ˆì´ì–´ë“¤\n",
        "        for i, gru_layer in enumerate(self.gru_layers):\n",
        "            x, _ = gru_layer(x)\n",
        "            if i < len(self.gru_layers) - 1:\n",
        "                x = x.transpose(1, 2)\n",
        "                x = self.batch_norms[i](x)\n",
        "                x = x.transpose(1, 2)\n",
        "                x = self.dropout(x)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ì‹œì  ì¶œë ¥\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Dense ë ˆì´ì–´ë“¤\n",
        "        for i, dense_layer in enumerate(self.dense_layers):\n",
        "            x = dense_layer(x)\n",
        "            x = self.dense_batch_norms[i](x)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # ë¶„ë¥˜ ì¶œë ¥ (ë¡œì§“)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class LeafClassificationTrainer:\n",
        "    def __init__(self, model, class_names=['Low', 'Medium', 'High']):\n",
        "        self.model = model.to(device)\n",
        "        self.class_names = class_names\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accs = []\n",
        "        self.val_accs = []\n",
        "\n",
        "    def train_epoch(self, train_loader, optimizer, criterion):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate_epoch(self, val_loader, criterion):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device).long()\n",
        "\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += batch_y.size(0)\n",
        "                correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val,\n",
        "              epochs=100, batch_size=32, learning_rate=0.001, patience=20):\n",
        "\n",
        "        print(f\"ğŸš€ ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "        print(f\"   í›ˆë ¨ ë°ì´í„°: {X_train.shape}\")\n",
        "        print(f\"   ê²€ì¦ ë°ì´í„°: {X_val.shape}\")\n",
        "\n",
        "        # ë°ì´í„° ë¡œë”\n",
        "        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
        "        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)\n",
        "        class_counts = np.bincount(y_train)\n",
        "        class_weights = len(y_train) / (len(class_counts) * class_counts)\n",
        "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=True)\n",
        "\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)\n",
        "            val_loss, val_acc = self.validate_epoch(val_loader, criterion)\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_accs.append(train_acc)\n",
        "            self.val_accs.append(val_acc)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "                print(f'  Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%')\n",
        "                print(f'  Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%')\n",
        "                print()\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), '/content/drive/MyDrive/mod/processed/best_classification_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"â° ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
        "                break\n",
        "\n",
        "        # ìµœê³  ëª¨ë¸ ë¡œë“œ\n",
        "        self.model.load_state_dict(torch.load('/content/drive/MyDrive/mod/processed/best_classification_model.pth'))\n",
        "        print(\"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œë¨\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        print(\"ğŸ“ˆ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_test_tensor)\n",
        "            _, y_pred = torch.max(outputs, 1)\n",
        "            y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "        # ì„±ëŠ¥ ë©”íŠ¸ë¦­\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"\\nğŸ“Š ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
        "        print(f\"   ì •í™•ë„ (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        # ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "        print(f\"\\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=self.class_names))\n",
        "\n",
        "        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=self.class_names, yticklabels=self.class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "        return accuracy, y_pred\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss ê·¸ë˜í”„\n",
        "        axes[0].plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        axes[0].plot(self.val_losses, label='Validation Loss', color='orange')\n",
        "        axes[0].set_title('Model Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy ê·¸ë˜í”„\n",
        "        axes[1].plot(self.train_accs, label='Training Accuracy', color='blue')\n",
        "        axes[1].plot(self.val_accs, label='Validation Accuracy', color='orange')\n",
        "        axes[1].set_title('Model Accuracy')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Accuracy (%)')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def prepare_classification_data(processor_new):\n",
        "    \"\"\"ë¶„ë¥˜ìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
        "\n",
        "    # ë¶„ë¥˜ ë ˆì´ë¸” ìƒì„±\n",
        "    class_names, q33, q67 = create_leaf_classification_data(processor_new)\n",
        "\n",
        "    # ë†ì¥ ë¶„í•  (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "    import numpy as np\n",
        "    farm_ids = processor_new.merged_data['farm_id'].unique()\n",
        "    np.random.seed(42)\n",
        "    shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "    n_train = int(len(farm_ids) * 0.7)\n",
        "    n_val = int(len(farm_ids) * 0.2)\n",
        "\n",
        "    train_farms = shuffled_farms[:n_train]\n",
        "    val_farms = shuffled_farms[n_train:n_train+n_val]\n",
        "    test_farms = shuffled_farms[n_train+n_val:]\n",
        "\n",
        "    # ë°ì´í„° ë¶„í• \n",
        "    train_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(train_farms)]\n",
        "    val_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(val_farms)]\n",
        "    test_data = processor_new.merged_data[processor_new.merged_data['farm_id'].isin(test_farms)]\n",
        "\n",
        "    # íŠ¹ì„± ì»¬ëŸ¼\n",
        "    feature_cols = [col for col in processor_new.merged_data.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "    # ì •ê·œí™”\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler_env = MinMaxScaler()\n",
        "\n",
        "    train_data[feature_cols] = scaler_env.fit_transform(train_data[feature_cols])\n",
        "    val_data[feature_cols] = scaler_env.transform(val_data[feature_cols])\n",
        "    test_data[feature_cols] = scaler_env.transform(test_data[feature_cols])\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    def create_classification_sequences(df, feature_cols, target_col, seq_len=3):\n",
        "        X, y = [], []\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "            if len(farm_df) <= seq_len:\n",
        "                continue\n",
        "            for i in range(seq_len, len(farm_df)):\n",
        "                X.append(farm_df[feature_cols].iloc[i-seq_len:i].values)\n",
        "                y.append(farm_df[target_col].iloc[i])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X_train, y_train = create_classification_sequences(train_data, feature_cols, 'leaf_class')\n",
        "    X_val, y_val = create_classification_sequences(val_data, feature_cols, 'leaf_class')\n",
        "    X_test, y_test = create_classification_sequences(test_data, feature_cols, 'leaf_class')\n",
        "\n",
        "    print(f\"\\nğŸ“Š ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names, scaler_env\n",
        "\n",
        "def main_classification_training(processor_new):\n",
        "    \"\"\"ë©”ì¸ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "\n",
        "    print(\"ğŸŒ± ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„\n",
        "    train_data, val_data, test_data, class_names, scaler_env = prepare_classification_data(processor_new)\n",
        "    X_train, y_train = train_data\n",
        "    X_val, y_val = val_data\n",
        "    X_test, y_test = test_data\n",
        "\n",
        "    # ëª¨ë¸ ìƒì„±\n",
        "    model = LeafClassificationGRU(\n",
        "        input_size=X_train.shape[2],\n",
        "        hidden_sizes=[64, 32],\n",
        "        dense_sizes=[16],\n",
        "        num_classes=3,\n",
        "        dropout_rate=0.2\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ¤– ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¡°:\")\n",
        "    print(f\"   ì…ë ¥ í¬ê¸°: {X_train.shape[2]}\")\n",
        "    print(f\"   í´ë˜ìŠ¤ ìˆ˜: 3 ({class_names})\")\n",
        "    print(f\"   ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í›ˆë ¨\n",
        "    trainer = LeafClassificationTrainer(model, class_names)\n",
        "    trainer.train(X_train, y_train, X_val, y_val, epochs=100, patience=20)\n",
        "\n",
        "    # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\n",
        "    trainer.plot_training_history()\n",
        "\n",
        "    # ëª¨ë¸ í‰ê°€\n",
        "    accuracy, predictions = trainer.evaluate(X_test, y_test)\n",
        "\n",
        "    print(f\"\\nğŸ‰ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“ˆ ìµœì¢… ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    return model, trainer, accuracy, class_names\n",
        "\n",
        "# ì‹¤í–‰ ì˜ˆì œ\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸŒ± ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(\"ğŸ“ ì‚¬ìš©ë²•:\")\n",
        "    print(\"   model, trainer, accuracy, class_names = main_classification_training(processor_new)\")"
      ],
      "metadata": {
        "id": "7qUJ641ROE_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì½”ë©ìš©)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "class SmartFarm14DataProcessor:\n",
        "    def __init__(self, data_path='/content/drive/MyDrive/mod'):\n",
        "        \"\"\"\n",
        "        14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            data_path (str): ë°ì´í„° íŒŒì¼ë“¤ì´ ìˆëŠ” ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.farm_data = {}  # ë†ì¥ë³„ í†µí•© ë°ì´í„°\n",
        "        self.train_farms = []\n",
        "        self.val_farms = []\n",
        "        self.test_farms = []\n",
        "\n",
        "        self.scaler_env = MinMaxScaler()\n",
        "        self.scaler_growth = MinMaxScaler()\n",
        "\n",
        "        # ì œê±°í•  í™˜ê²½ ì„¼ì„œ (ì§€ì˜¨, í’í–¥, í’ì† ê´€ë ¨)\n",
        "        self.excluded_env_sensors = [\n",
        "            'ì–‘ì•¡-ì§€ì˜¨', 'ì–‘ì•¡-ì§€ìŠµ', 'ì™¸ë¶€-ì™¸ë¶€í’í–¥', 'ì™¸ë¶€-ì™¸ë¶€í’ì†'\n",
        "        ]\n",
        "\n",
        "        # ì‚¬ìš©í•  í™˜ê²½ ë³€ìˆ˜ë§Œ ì •ì˜ (ì§€ì˜¨, ë°”ëŒ ê´€ë ¨ ì œì™¸)\n",
        "        self.env_columns_mapping = {\n",
        "            'ë‚´ë¶€-ë‚´ë¶€CO2': 'internal_co2',\n",
        "            'ë‚´ë¶€-ë‚´ë¶€ìŠµë„': 'internal_humidity',\n",
        "            'ë‚´ë¶€-ë‚´ë¶€ì¼ì‚¬ëŸ‰': 'internal_solar',\n",
        "            'ì™¸ë¶€-ì™¸ë¶€ì¼ì‚¬ëŸ‰': 'external_solar',\n",
        "            'ì™¸ë¶€-ì™¸ë¶€ì˜¨ë„': 'external_temp',\n",
        "            'ë‚´ë¶€-ë‚´ë¶€ì˜¨ë„': 'internal_temp'\n",
        "        }\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜: ì—½ìˆ˜ì™€ ìƒì¥ê¸¸ì´ë§Œ ì‚¬ìš©\n",
        "        self.target_columns_mapping = {\n",
        "            'ì—½ìˆ˜(ê°œ)': 'leaf_number',\n",
        "            'ìƒì¥ê¸¸ì´(mm)': 'growth_length'\n",
        "        }\n",
        "\n",
        "        # ì¶”í›„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°íƒ€ ìƒì¥ ë³€ìˆ˜ë“¤ (ê½ƒ ê´€ë ¨ í¬í•¨)\n",
        "        self.other_growth_columns = {\n",
        "            'ì´ˆì¥(mm)': 'plant_height',\n",
        "            'ì—½ì¥(mm)': 'leaf_length',\n",
        "            'ì—½í­(mm)': 'leaf_width',\n",
        "            'ì¤„ê¸°ì§ê²½(mm)': 'stem_diameter',\n",
        "            'í™”ë°©ë†’ì´(mm)': 'flower_height',  # ê½ƒ ê´€ë ¨\n",
        "            'ì°©ê³¼ìˆ˜(ê°œ)': 'fruit_count',\n",
        "            'ê°œí™”êµ°(ì )': 'flower_position',  # ê½ƒ ê´€ë ¨\n",
        "            'ì°©ê³¼êµ°(ì )': 'fruit_position',\n",
        "            'ìˆ˜í™•êµ°(ì )': 'harvest_position'\n",
        "        }\n",
        "\n",
        "    def load_all_farm_data(self):\n",
        "        \"\"\"14ê°œ ë†ì¥ì˜ ëª¨ë“  í™˜ê²½ ë° ìƒìœ¡ ë°ì´í„° ë¡œë“œ\"\"\"\n",
        "        print(\"ğŸšœ 14ê°œ ë†ì¥ ë°ì´í„° ë¡œë”© ì‹œì‘...\")\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        for farm_id in range(1, 13):  # en1~en14, gr1~gr14\n",
        "            print(f\"\\n--- ë†ì¥ {farm_id} ë°ì´í„° ì²˜ë¦¬ ---\")\n",
        "\n",
        "            env_file = f\"{self.data_path}/en{farm_id}.xlsx\"\n",
        "            growth_file = f\"{self.data_path}/gr{farm_id}.xlsx\"\n",
        "\n",
        "            if os.path.exists(env_file) and os.path.exists(growth_file):\n",
        "                try:\n",
        "                    # í™˜ê²½ ë°ì´í„° ë¡œë“œ\n",
        "                    env_df = pd.read_excel(env_file, skiprows=3)\n",
        "                    env_clean = self._clean_env_data(env_df)\n",
        "\n",
        "                    # ìƒìœ¡ ë°ì´í„° ë¡œë“œ\n",
        "                    growth_df = pd.read_excel(growth_file, skiprows=3)\n",
        "                    growth_clean = self._clean_growth_data(growth_df)\n",
        "\n",
        "                    # ë†ì¥ë³„ ë°ì´í„° ë³‘í•©\n",
        "                    merged_farm_data = self._merge_farm_data(env_clean, growth_clean, farm_id)\n",
        "\n",
        "                    if not merged_farm_data.empty:\n",
        "                        self.farm_data[f'farm_{farm_id}'] = merged_farm_data\n",
        "                        success_count += 1\n",
        "                        print(f\"âœ… ë†ì¥ {farm_id}: {len(merged_farm_data)}ì£¼ì°¨ ë°ì´í„°\")\n",
        "                    else:\n",
        "                        print(f\"âš ï¸  ë†ì¥ {farm_id}: ë³‘í•© í›„ ë°ì´í„° ì—†ìŒ\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ ë†ì¥ {farm_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "            else:\n",
        "                missing_files = []\n",
        "                if not os.path.exists(env_file): missing_files.append(f\"en{farm_id}.xlsx\")\n",
        "                if not os.path.exists(growth_file): missing_files.append(f\"gr{farm_id}.xlsx\")\n",
        "                print(f\"âš ï¸  ë†ì¥ {farm_id}: {', '.join(missing_files)} íŒŒì¼ ì—†ìŒ\")\n",
        "\n",
        "        print(f\"\\nğŸ‰ ë¡œë”© ì™„ë£Œ: {success_count}/12ê°œ ë†ì¥ ì„±ê³µ\")\n",
        "        return success_count\n",
        "\n",
        "    def _clean_env_data(self, df):\n",
        "        \"\"\"í™˜ê²½ ë°ì´í„° ì •ë¦¬ (ì§€ì˜¨, ë°”ëŒ ê´€ë ¨ ì œê±°)\"\"\"\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # ì£¼ì°¨ ì •ë³´ ì¶”ì¶œ\n",
        "        week_col = df.columns[0]\n",
        "        if 'ì£¼ì°¨' in str(df[week_col].iloc[0]) if len(df) > 0 else False:\n",
        "            df['week'] = df[week_col].str.extract(r'(\\d+)').astype(float)\n",
        "        else:\n",
        "            df['week'] = pd.to_numeric(df[week_col], errors='coerce')\n",
        "\n",
        "        # ì‚¬ìš©í•  í™˜ê²½ ë³€ìˆ˜ë§Œ ì„ íƒ (ì§€ì˜¨, ë°”ëŒ ê´€ë ¨ ì œì™¸)\n",
        "        numeric_cols = []\n",
        "        for col in df.columns[1:]:\n",
        "            if col in self.env_columns_mapping:\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                    numeric_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°\n",
        "        df = df.dropna(subset=['week'] + numeric_cols)\n",
        "\n",
        "        # ì»¬ëŸ¼ëª… ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½\n",
        "        rename_dict = {'week': 'week'}\n",
        "        for old_col, new_col in self.env_columns_mapping.items():\n",
        "            if old_col in df.columns:\n",
        "                rename_dict[old_col] = new_col\n",
        "\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        keep_cols = ['week'] + [v for k, v in self.env_columns_mapping.items() if k in df.columns]\n",
        "        available_cols = [col for col in keep_cols if col in df.columns]\n",
        "        df = df[available_cols]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _clean_growth_data(self, df):\n",
        "        \"\"\"ìƒìœ¡ ë°ì´í„° ì •ë¦¬ (ì—½ìˆ˜ì™€ ìƒì¥ê¸¸ì´ë§Œ ì¶”ì¶œ)\"\"\"\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # ì£¼ì°¨ ì •ë³´ ì¶”ì¶œ\n",
        "        if 'ì£¼ì°¨' in df.columns:\n",
        "            df['week'] = pd.to_numeric(df['ì£¼ì°¨'], errors='coerce')\n",
        "        else:\n",
        "            df['week'] = range(1, len(df) + 1)\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ë“¤ë§Œ ì²˜ë¦¬ (ì—½ìˆ˜, ìƒì¥ê¸¸ì´)\n",
        "        numeric_cols = []\n",
        "        for col in df.columns:\n",
        "            if col in self.target_columns_mapping:\n",
        "                try:\n",
        "                    df[col] = df[col].replace([' ', ''], 0)\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "                    numeric_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # ë‹¤ë¥¸ ìƒì¥ ë³€ìˆ˜ë“¤ë„ ì €ì¥ (ì¶”í›„ ê½ƒ ì˜ˆì¸¡ìš©)\n",
        "        other_cols = []\n",
        "        for col in df.columns:\n",
        "            if col in self.other_growth_columns:\n",
        "                try:\n",
        "                    df[col] = df[col].replace([' ', ''], 0)\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "                    other_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        df = df.dropna(subset=['week'])\n",
        "\n",
        "        # ì»¬ëŸ¼ëª… ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½\n",
        "        rename_dict = {'week': 'week'}\n",
        "        rename_dict.update(self.target_columns_mapping)\n",
        "        rename_dict.update(self.other_growth_columns)\n",
        "\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        target_cols = list(self.target_columns_mapping.values())\n",
        "        other_cols_eng = list(self.other_growth_columns.values())\n",
        "        keep_cols = ['week'] + target_cols + other_cols_eng\n",
        "        available_cols = [col for col in keep_cols if col in df.columns]\n",
        "        df = df[available_cols]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _merge_farm_data(self, env_df, growth_df, farm_id):\n",
        "        \"\"\"ë†ì¥ë³„ í™˜ê²½-ìƒìœ¡ ë°ì´í„° ë³‘í•©\"\"\"\n",
        "        if env_df.empty or growth_df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # ì£¼ì°¨ë³„ ë³‘í•©\n",
        "        merged = pd.merge(env_df, growth_df, on='week', how='inner')\n",
        "\n",
        "        # ë†ì¥ ID ì¶”ê°€\n",
        "        merged['farm_id'] = farm_id\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def split_farms_for_validation(self, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_state=42):\n",
        "        \"\"\"ë†ì¥ì„ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• \"\"\"\n",
        "        farm_ids = list(self.farm_data.keys())\n",
        "        n_farms = len(farm_ids)\n",
        "\n",
        "        if n_farms < 3:\n",
        "            print(\"âŒ ìµœì†Œ 3ê°œ ë†ì¥ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "            return\n",
        "\n",
        "        np.random.seed(random_state)\n",
        "        shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "        n_train = max(1, int(n_farms * train_ratio))\n",
        "        n_val = max(1, int(n_farms * val_ratio))\n",
        "        n_test = n_farms - n_train - n_val\n",
        "\n",
        "        self.train_farms = shuffled_farms[:n_train].tolist()\n",
        "        self.val_farms = shuffled_farms[n_train:n_train+n_val].tolist()\n",
        "        self.test_farms = shuffled_farms[n_train+n_val:].tolist()\n",
        "\n",
        "        print(f\"\\nğŸ¯ ë†ì¥ ë¶„í•  ê²°ê³¼:\")\n",
        "        print(f\"   í›ˆë ¨ìš©: {len(self.train_farms)}ê°œ ë†ì¥ - {self.train_farms}\")\n",
        "        print(f\"   ê²€ì¦ìš©: {len(self.val_farms)}ê°œ ë†ì¥ - {self.val_farms}\")\n",
        "        print(f\"   í…ŒìŠ¤íŠ¸ìš©: {len(self.test_farms)}ê°œ ë†ì¥ - {self.test_farms}\")\n",
        "\n",
        "    def prepare_datasets(self):\n",
        "        \"\"\"í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\"\"\"\n",
        "        if not self.train_farms:\n",
        "            print(\"âŒ ë¨¼ì € split_farms_for_validation()ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(\"\\nğŸ“¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...\")\n",
        "\n",
        "        # ê° ì„¸íŠ¸ë³„ ë°ì´í„° í†µí•©\n",
        "        train_data = pd.concat([self.farm_data[farm] for farm in self.train_farms], ignore_index=True)\n",
        "        val_data = pd.concat([self.farm_data[farm] for farm in self.val_farms], ignore_index=True)\n",
        "        test_data = pd.concat([self.farm_data[farm] for farm in self.test_farms], ignore_index=True)\n",
        "\n",
        "        print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}í–‰ ({len(self.train_farms)}ê°œ ë†ì¥)\")\n",
        "        print(f\"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data)}í–‰ ({len(self.val_farms)}ê°œ ë†ì¥)\")\n",
        "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}í–‰ ({len(self.test_farms)}ê°œ ë†ì¥)\")\n",
        "\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def add_lag_features(self, df, lag_weeks=[1, 2]):\n",
        "        \"\"\"ì‹œê°„ ì§€ì—° íŠ¹ì„± ì¶”ê°€\"\"\"\n",
        "        print(f\"â° ì§€ì—° íŠ¹ì„± ì¶”ê°€: {lag_weeks}ì£¼ ì§€ì—°\")\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì§€ì—° íŠ¹ì„± ìƒì„±\n",
        "        env_cols = [col for col in df.columns if col.startswith(('internal_', 'external_'))]\n",
        "\n",
        "        # ë†ì¥ë³„ë¡œ ì§€ì—° íŠ¹ì„± ìƒì„± (ë†ì¥ ê°„ ë°ì´í„° ì„ì„ ë°©ì§€)\n",
        "        df_with_lag = []\n",
        "\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].copy().sort_values('week')\n",
        "\n",
        "            for lag in lag_weeks:\n",
        "                for col in env_cols:\n",
        "                    lag_col_name = f\"{col}_lag{lag}\"\n",
        "                    farm_df[lag_col_name] = farm_df[col].shift(lag)\n",
        "\n",
        "            # ì§€ì—° íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ ê²°ì¸¡ì¹˜ ì œê±°\n",
        "            farm_df = farm_df.dropna()\n",
        "            df_with_lag.append(farm_df)\n",
        "\n",
        "        result_df = pd.concat(df_with_lag, ignore_index=True)\n",
        "\n",
        "        lag_cols_count = len([col for col in result_df.columns if 'lag' in col])\n",
        "        print(f\"âœ… ì§€ì—° íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ: {lag_cols_count}ê°œ\")\n",
        "\n",
        "        return result_df\n",
        "\n",
        "    def add_derived_features(self, df):\n",
        "        \"\"\"íŒŒìƒ ë³€ìˆ˜ ìƒì„±\"\"\"\n",
        "        print(\"ğŸ”§ íŒŒìƒ ë³€ìˆ˜ ìƒì„±...\")\n",
        "\n",
        "        # ì˜¨ë„ ì°¨ì´\n",
        "        if 'external_temp' in df.columns and 'internal_temp' in df.columns:\n",
        "            df['temp_diff'] = df['external_temp'] - df['internal_temp']\n",
        "\n",
        "        # ì¼ì‚¬ íš¨ìœ¨\n",
        "        if 'internal_solar' in df.columns and 'external_solar' in df.columns:\n",
        "            df['solar_efficiency'] = np.where(\n",
        "                df['external_solar'] > 0,\n",
        "                df['internal_solar'] / df['external_solar'],\n",
        "                0\n",
        "            )\n",
        "\n",
        "        # ì˜¨ìŠµë„ ì§€ìˆ˜\n",
        "        if 'internal_temp' in df.columns and 'internal_humidity' in df.columns:\n",
        "            df['temp_humidity_index'] = df['internal_temp'] * df['internal_humidity']\n",
        "\n",
        "        # ê³„ì ˆì„± íŠ¹ì„±\n",
        "        df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
        "        df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
        "\n",
        "        print(\"âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\")\n",
        "        return df\n",
        "\n",
        "    def normalize_data(self, train_df, val_df, test_df):\n",
        "        \"\"\"ë°ì´í„° ì •ê·œí™” (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§)\"\"\"\n",
        "        print(\"ğŸ“ ë°ì´í„° ì •ê·œí™”...\")\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ë“¤\n",
        "        env_cols = [col for col in train_df.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ë“¤ (ì—½ìˆ˜, ìƒì¥ê¸¸ì´)\n",
        "        target_cols = ['leaf_number', 'growth_length']\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ ì •ê·œí™”\n",
        "        if env_cols:\n",
        "            train_df[env_cols] = self.scaler_env.fit_transform(train_df[env_cols])\n",
        "            val_df[env_cols] = self.scaler_env.transform(val_df[env_cols])\n",
        "            test_df[env_cols] = self.scaler_env.transform(test_df[env_cols])\n",
        "            print(f\"âœ… í™˜ê²½ ë³€ìˆ˜ ì •ê·œí™”: {len(env_cols)}ê°œ\")\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ ì •ê·œí™”\n",
        "        available_targets = [col for col in target_cols if col in train_df.columns]\n",
        "        if available_targets:\n",
        "            train_df[available_targets] = self.scaler_growth.fit_transform(train_df[available_targets])\n",
        "            val_df[available_targets] = self.scaler_growth.transform(val_df[available_targets])\n",
        "            test_df[available_targets] = self.scaler_growth.transform(test_df[available_targets])\n",
        "            print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ì •ê·œí™”: {len(available_targets)}ê°œ\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    def create_sequences(self, df, sequence_length=3):\n",
        "        \"\"\"GRUìš© ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ë†ì¥ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬)\"\"\"\n",
        "        print(f\"ğŸ”„ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ê¸¸ì´: {sequence_length}ì£¼)\")\n",
        "\n",
        "        # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col.startswith(('internal_', 'external_')) or\n",
        "                          'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                          'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "        # íƒ€ê²Ÿ ì»¬ëŸ¼\n",
        "        target_cols = ['leaf_number', 'growth_length']\n",
        "        available_targets = [col for col in target_cols if col in df.columns]\n",
        "\n",
        "        if not available_targets:\n",
        "            print(f\"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_cols}\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"ğŸ“Š íŠ¹ì„± ë³€ìˆ˜: {len(feature_cols)}ê°œ\")\n",
        "        print(f\"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {len(available_targets)}ê°œ - {available_targets}\")\n",
        "\n",
        "        # ë†ì¥ë³„ë¡œ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "        X_list, y_list = [], []\n",
        "\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "\n",
        "            if len(farm_df) <= sequence_length:\n",
        "                continue\n",
        "\n",
        "            for i in range(sequence_length, len(farm_df)):\n",
        "                # ê³¼ê±° sequence_length ì£¼ê°„ì˜ í™˜ê²½ ë°ì´í„°\n",
        "                X_list.append(farm_df[feature_cols].iloc[i-sequence_length:i].values)\n",
        "                # í˜„ì¬ ì£¼ì˜ íƒ€ê²Ÿ ê°’\n",
        "                y_list.append(farm_df[available_targets].iloc[i].values)\n",
        "\n",
        "        if not X_list:\n",
        "            print(\"âŒ ìƒì„±ëœ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return None, None\n",
        "\n",
        "        X = np.array(X_list)\n",
        "        y = np.array(y_list)\n",
        "\n",
        "        print(f\"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
        "        print(f\"   - X shape: {X.shape} (samples, time_steps, features)\")\n",
        "        print(f\"   - y shape: {y.shape} (samples, targets)\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def save_processed_data(self, train_df, val_df, test_df, save_path='/content/drive/MyDrive/mod/processed'):\n",
        "        \"\"\"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\"\"\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # CSVë¡œ ì €ì¥\n",
        "        train_df.to_csv(f\"{save_path}/train_data.csv\", index=False, encoding='utf-8-sig')\n",
        "        val_df.to_csv(f\"{save_path}/val_data.csv\", index=False, encoding='utf-8-sig')\n",
        "        test_df.to_csv(f\"{save_path}/test_data.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
        "        import joblib\n",
        "        joblib.dump(self.scaler_env, f\"{save_path}/scaler_env.pkl\")\n",
        "        joblib.dump(self.scaler_growth, f\"{save_path}/scaler_growth.pkl\")\n",
        "\n",
        "        # ë†ì¥ ë¶„í•  ì •ë³´ ì €ì¥\n",
        "        farm_split_info = {\n",
        "            'train_farms': self.train_farms,\n",
        "            'val_farms': self.val_farms,\n",
        "            'test_farms': self.test_farms\n",
        "        }\n",
        "        import json\n",
        "        with open(f\"{save_path}/farm_split.json\", 'w') as f:\n",
        "            json.dump(farm_split_info, f, indent=2)\n",
        "\n",
        "        # ë°ì´í„° ì •ë³´ ì €ì¥\n",
        "        with open(f\"{save_path}/data_info.txt\", 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ì „ì²˜ë¦¬ ë°ì´í„° ì •ë³´\\n\")\n",
        "            f.write(f\"ìƒì„± ì¼ì‹œ: {pd.Timestamp.now()}\\n\\n\")\n",
        "            f.write(f\"ë†ì¥ ë¶„í• :\\n\")\n",
        "            f.write(f\"  í›ˆë ¨: {self.train_farms}\\n\")\n",
        "            f.write(f\"  ê²€ì¦: {self.val_farms}\\n\")\n",
        "            f.write(f\"  í…ŒìŠ¤íŠ¸: {self.test_farms}\\n\\n\")\n",
        "            f.write(f\"ë°ì´í„° í¬ê¸°:\\n\")\n",
        "            f.write(f\"  í›ˆë ¨: {train_df.shape}\\n\")\n",
        "            f.write(f\"  ê²€ì¦: {val_df.shape}\\n\")\n",
        "            f.write(f\"  í…ŒìŠ¤íŠ¸: {test_df.shape}\\n\\n\")\n",
        "            f.write(f\"íƒ€ê²Ÿ ë³€ìˆ˜: ì—½ìˆ˜(leaf_number), ìƒì¥ê¸¸ì´(growth_length)\\n\")\n",
        "            f.write(f\"ì œì™¸ëœ ì„¼ì„œ: {self.excluded_env_sensors}\\n\")\n",
        "\n",
        "        print(f\"\\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "    def plot_farm_overview(self, train_df, val_df, test_df):\n",
        "        \"\"\"ë†ì¥ë³„ ë°ì´í„° ê°œìš” ì‹œê°í™”\"\"\"\n",
        "        print(\"ğŸ“Š ë†ì¥ë³„ ë°ì´í„° ì‹œê°í™”...\")\n",
        "\n",
        "        # ë†ì¥ë³„ ë°ì´í„° ë¶„í¬\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # 1. ë†ì¥ë³„ ë°ì´í„° ê°œìˆ˜\n",
        "        all_data = pd.concat([\n",
        "            train_df.assign(split='Train'),\n",
        "            val_df.assign(split='Validation'),\n",
        "            test_df.assign(split='Test')\n",
        "        ])\n",
        "\n",
        "        farm_counts = all_data.groupby(['farm_id', 'split']).size().unstack(fill_value=0)\n",
        "        farm_counts.plot(kind='bar', stacked=True, ax=axes[0,0])\n",
        "        axes[0,0].set_title('ë†ì¥ë³„ ë°ì´í„° ë¶„í¬')\n",
        "        axes[0,0].set_xlabel('ë†ì¥ ID')\n",
        "        axes[0,0].set_ylabel('ë°ì´í„° ê°œìˆ˜')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. ì—½ìˆ˜ ë¶„í¬\n",
        "        axes[0,1].hist([train_df['leaf_number'], val_df['leaf_number'], test_df['leaf_number']],\n",
        "                      bins=20, alpha=0.7, label=['Train', 'Val', 'Test'])\n",
        "        axes[0,1].set_title('ì—½ìˆ˜ ë¶„í¬')\n",
        "        axes[0,1].set_xlabel('ì—½ìˆ˜')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # 3. ìƒì¥ê¸¸ì´ ë¶„í¬\n",
        "        axes[1,0].hist([train_df['growth_length'], val_df['growth_length'], test_df['growth_length']],\n",
        "                      bins=20, alpha=0.7, label=['Train', 'Val', 'Test'])\n",
        "        axes[1,0].set_title('ìƒì¥ê¸¸ì´ ë¶„í¬')\n",
        "        axes[1,0].set_xlabel('ìƒì¥ê¸¸ì´')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 4. ì£¼ì°¨ë³„ í‰ê·  íƒ€ê²Ÿ ê°’\n",
        "        week_stats = all_data.groupby('week')[['leaf_number', 'growth_length']].mean()\n",
        "        week_stats.plot(ax=axes[1,1])\n",
        "        axes[1,1].set_title('ì£¼ì°¨ë³„ í‰ê·  íƒ€ê²Ÿ ê°’')\n",
        "        axes[1,1].set_xlabel('ì£¼ì°¨')\n",
        "        axes[1,1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def main():\n",
        "    \"\"\"14ê°œ ë†ì¥ ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
        "    print(\"ğŸŒ± 14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘\")\n",
        "    print(\"ğŸ¯ íƒ€ê²Ÿ: ì—½ìˆ˜(leaf_number) + ìƒì¥ê¸¸ì´(growth_length)\")\n",
        "    print(\"ğŸš« ì œì™¸ ì„¼ì„œ: ì§€ì˜¨, í’í–¥, í’ì†\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”\n",
        "    processor = SmartFarm14DataProcessor()\n",
        "\n",
        "    # 1. 14ê°œ ë†ì¥ ë°ì´í„° ë¡œë“œ\n",
        "    success_count = processor.load_all_farm_data()\n",
        "\n",
        "    if success_count < 3:\n",
        "        print(\"âŒ ìµœì†Œ 3ê°œ ë†ì¥ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # 2. ë†ì¥ì„ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• \n",
        "    processor.split_farms_for_validation(train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)\n",
        "\n",
        "    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "    train_data, val_data, test_data = processor.prepare_datasets()\n",
        "\n",
        "    # 4. ì§€ì—° íŠ¹ì„± ì¶”ê°€\n",
        "    train_data = processor.add_lag_features(train_data, lag_weeks=[1, 2])\n",
        "    val_data = processor.add_lag_features(val_data, lag_weeks=[1, 2])\n",
        "    test_data = processor.add_lag_features(test_data, lag_weeks=[1, 2])\n",
        "\n",
        "    # 5. íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
        "    train_data = processor.add_derived_features(train_data)\n",
        "    val_data = processor.add_derived_features(val_data)\n",
        "    test_data = processor.add_derived_features(test_data)\n",
        "\n",
        "    # 6. ë°ì´í„° ì •ê·œí™”\n",
        "    train_data, val_data, test_data = processor.normalize_data(train_data, val_data, test_data)\n",
        "\n",
        "    # 7. ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    X_train, y_train = processor.create_sequences(train_data, sequence_length=3)\n",
        "    X_val, y_val = processor.create_sequences(val_data, sequence_length=3)\n",
        "    X_test, y_test = processor.create_sequences(test_data, sequence_length=3)\n",
        "\n",
        "    # 8. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
        "    processor.save_processed_data(train_data, val_data, test_data)\n",
        "\n",
        "    # 9. ë°ì´í„° ê°œìš” ì‹œê°í™”\n",
        "    processor.plot_farm_overview(train_data, val_data, test_data)\n",
        "\n",
        "    print(\"\\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š ìµœì¢… ì‹œí€€ìŠ¤ ë°ì´í„°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape if X_train is not None else 'None'}, y{y_train.shape if y_train is not None else 'None'}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape if X_val is not None else 'None'}, y{y_val.shape if y_val is not None else 'None'}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape if X_test is not None else 'None'}, y{y_test.shape if y_test is not None else 'None'}\")\n",
        "\n",
        "    return processor, (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    processor, train_data, val_data, test_data = main()"
      ],
      "metadata": {
        "id": "hCy082q5OgGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì½”ë©ìš©)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "class SmartFarm14DataProcessor:\n",
        "    def __init__(self, data_path='/content/drive/MyDrive/mod'):\n",
        "        \"\"\"\n",
        "        14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            data_path (str): ë°ì´í„° íŒŒì¼ë“¤ì´ ìˆëŠ” ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.farm_data = {}  # ë†ì¥ë³„ í†µí•© ë°ì´í„°\n",
        "        self.train_farms = []\n",
        "        self.val_farms = []\n",
        "        self.test_farms = []\n",
        "\n",
        "        self.scaler_env = MinMaxScaler()\n",
        "        self.scaler_growth = MinMaxScaler()\n",
        "\n",
        "        # ì œê±°í•  í™˜ê²½ ì„¼ì„œ (ì§€ì˜¨, í’í–¥, í’ì† ê´€ë ¨)\n",
        "        self.excluded_env_sensors = [\n",
        "            'ì–‘ì•¡-ì§€ì˜¨', 'ì–‘ì•¡-ì§€ìŠµ', 'ì™¸ë¶€-ì™¸ë¶€í’í–¥', 'ì™¸ë¶€-ì™¸ë¶€í’ì†'\n",
        "        ]\n",
        "\n",
        "        # ì‚¬ìš©í•  í™˜ê²½ ë³€ìˆ˜ë§Œ ì„ íƒ - ì˜ë¬¸ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •\n",
        "        self.env_columns_mapping_eng = {\n",
        "            'InternalEnvironment_CarbonDioxide': 'internal_co2',\n",
        "            'InternalEnvironment_Humidity': 'internal_humidity',\n",
        "            'InternalEnvironment_Insolation': 'internal_solar',\n",
        "            'ExternalEnvironment_Insolation': 'external_solar',\n",
        "            'ExternalEnvironment_Temperature': 'external_temp',\n",
        "            'InternalEnvironment_Temperature': 'internal_temp'\n",
        "        }\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜: ì˜ë¬¸ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •\n",
        "        self.target_columns_mapping_eng = {\n",
        "            'LeafNumber': 'leaf_number',\n",
        "            'GrowthLength': 'growth_length'\n",
        "        }\n",
        "\n",
        "        # ì¶”í›„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°íƒ€ ìƒì¥ ë³€ìˆ˜ë“¤ (ì˜ë¬¸ í—¤ë” ê¸°ì¤€)\n",
        "        self.other_growth_columns_eng = {\n",
        "            'PlantHeight': 'plant_height',\n",
        "            'LeafLength': 'leaf_length',\n",
        "            'LeafWidth': 'leaf_width',\n",
        "            'StemDiameter': 'stem_diameter',\n",
        "            'FlowerClusterTop': 'flower_height',  # ê½ƒ ê´€ë ¨\n",
        "            'FruitingNumber': 'fruit_count',\n",
        "            'FlowerPosition': 'flower_position',  # ê½ƒ ê´€ë ¨\n",
        "            'FruitsPosition': 'fruit_position',\n",
        "            'HarvestPosition': 'harvest_position'\n",
        "        }\n",
        "\n",
        "    def load_all_farm_data(self):\n",
        "        \"\"\"14ê°œ ë†ì¥ì˜ ëª¨ë“  í™˜ê²½ ë° ìƒìœ¡ ë°ì´í„° ë¡œë“œ\"\"\"\n",
        "        print(\"ğŸšœ 14ê°œ ë†ì¥ ë°ì´í„° ë¡œë”© ì‹œì‘...\")\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        for farm_id in range(1, 15):  # en1~en14, gr1~gr14\n",
        "            print(f\"\\n--- ë†ì¥ {farm_id} ë°ì´í„° ì²˜ë¦¬ ---\")\n",
        "\n",
        "            env_file = f\"{self.data_path}/en{farm_id}.xlsx\"\n",
        "            growth_file = f\"{self.data_path}/gr{farm_id}.xlsx\"\n",
        "\n",
        "            if os.path.exists(env_file) and os.path.exists(growth_file):\n",
        "                try:\n",
        "                    # í™˜ê²½ ë°ì´í„° ë¡œë“œ\n",
        "                    print(f\"í™˜ê²½ íŒŒì¼ ë¡œë”©: {env_file}\")\n",
        "                    env_df = pd.read_excel(env_file, skiprows=2)  # 3í–‰ì§¸ë¶€í„° ì½ê¸° (ì˜ë¬¸ í—¤ë”)\n",
        "                    env_clean = self._clean_env_data(env_df, farm_id)\n",
        "                    print(f\"í™˜ê²½ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {env_clean.shape}\")\n",
        "\n",
        "                    # ìƒìœ¡ ë°ì´í„° ë¡œë“œ\n",
        "                    print(f\"ìƒìœ¡ íŒŒì¼ ë¡œë”©: {growth_file}\")\n",
        "                    growth_df = pd.read_excel(growth_file, skiprows=2)  # 3í–‰ì§¸ë¶€í„° ì½ê¸° (ì˜ë¬¸ í—¤ë”)\n",
        "                    growth_clean = self._clean_growth_data(growth_df, farm_id)\n",
        "                    print(f\"ìƒìœ¡ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {growth_clean.shape}\")\n",
        "\n",
        "                    # ë†ì¥ë³„ ë°ì´í„° ë³‘í•©\n",
        "                    merged_farm_data = self._merge_farm_data(env_clean, growth_clean, farm_id)\n",
        "                    print(f\"ë³‘í•© í›„ ë°ì´í„°: {merged_farm_data.shape}\")\n",
        "                    print(f\"ë³‘í•© í›„ ì»¬ëŸ¼: {list(merged_farm_data.columns)}\")\n",
        "\n",
        "                    if not merged_farm_data.empty and len(merged_farm_data.columns) > 4:  # week, farm_id ì™¸ì— ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "                        self.farm_data[f'farm_{farm_id}'] = merged_farm_data\n",
        "                        success_count += 1\n",
        "                        print(f\"âœ… ë†ì¥ {farm_id}: {len(merged_farm_data)}ì£¼ì°¨ ë°ì´í„°\")\n",
        "                    else:\n",
        "                        print(f\"âš ï¸  ë†ì¥ {farm_id}: ë³‘í•© í›„ ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì»¬ëŸ¼ ë¶€ì¡±\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ ë†ì¥ {farm_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "            else:\n",
        "                missing_files = []\n",
        "                if not os.path.exists(env_file): missing_files.append(f\"en{farm_id}.xlsx\")\n",
        "                if not os.path.exists(growth_file): missing_files.append(f\"gr{farm_id}.xlsx\")\n",
        "                print(f\"âš ï¸  ë†ì¥ {farm_id}: {', '.join(missing_files)} íŒŒì¼ ì—†ìŒ\")\n",
        "\n",
        "        print(f\"\\nğŸ‰ ë¡œë”© ì™„ë£Œ: {success_count}/14ê°œ ë†ì¥ ì„±ê³µ\")\n",
        "        return success_count\n",
        "\n",
        "    def _clean_env_data(self, df, farm_id):\n",
        "        \"\"\"í™˜ê²½ ë°ì´í„° ì •ë¦¬ (ì§€ì˜¨, ë°”ëŒ ê´€ë ¨ ì œê±°)\"\"\"\n",
        "        print(f\"ë†ì¥ {farm_id} í™˜ê²½ ë°ì´í„° ì •ë¦¬ ì‹œì‘\")\n",
        "        print(f\"ì›ë³¸ í™˜ê²½ ì»¬ëŸ¼ë“¤: {list(df.columns)}\")\n",
        "\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì£¼ì°¨ ì •ë³´ (ì˜ˆ: '01ì£¼ì°¨', '02ì£¼ì°¨')\n",
        "        week_col = df.columns[0]\n",
        "        df['week'] = df[week_col].str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "        # ì˜ë¬¸ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬\n",
        "        numeric_cols = []\n",
        "        for col in df.columns[1:]:\n",
        "            if col in self.env_columns_mapping_eng:\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                    numeric_cols.append(col)\n",
        "                    print(f\"í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬: {col}\")\n",
        "                except:\n",
        "                    print(f\"í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬ ì‹¤íŒ¨: {col}\")\n",
        "                    continue\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°\n",
        "        df = df.dropna(subset=['week'] + numeric_cols)\n",
        "\n",
        "        # ì»¬ëŸ¼ëª… ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½\n",
        "        rename_dict = {'week': 'week'}\n",
        "        for old_col, new_col in self.env_columns_mapping_eng.items():\n",
        "            if old_col in df.columns:\n",
        "                rename_dict[old_col] = new_col\n",
        "\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        keep_cols = ['week'] + [v for k, v in self.env_columns_mapping_eng.items() if k in df.columns]\n",
        "        available_cols = [col for col in keep_cols if col in df.columns]\n",
        "        df = df[available_cols]\n",
        "\n",
        "        print(f\"í™˜ê²½ ë°ì´í„° ìµœì¢… ì»¬ëŸ¼: {list(df.columns)}\")\n",
        "        print(f\"í™˜ê²½ ë°ì´í„° ìµœì¢… í¬ê¸°: {df.shape}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _clean_growth_data(self, df, farm_id):\n",
        "        \"\"\"ìƒìœ¡ ë°ì´í„° ì •ë¦¬ (ì—½ìˆ˜ì™€ ìƒì¥ê¸¸ì´ë§Œ ì¶”ì¶œ)\"\"\"\n",
        "        print(f\"ë†ì¥ {farm_id} ìƒìœ¡ ë°ì´í„° ì •ë¦¬ ì‹œì‘\")\n",
        "        print(f\"ì›ë³¸ ìƒìœ¡ ì»¬ëŸ¼ë“¤: {list(df.columns)}\")\n",
        "\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì´ ì£¼ì°¨ ì •ë³´ (ì²« ë²ˆì§¸ëŠ” ë‚ ì§œ)\n",
        "        week_col = df.columns[1]  # 'ì£¼ì°¨' ì»¬ëŸ¼ì€ ë‘ ë²ˆì§¸\n",
        "        df['week'] = pd.to_numeric(df[week_col], errors='coerce')\n",
        "\n",
        "        # ì˜ë¬¸ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬\n",
        "        target_found = []\n",
        "        for col in df.columns:\n",
        "            if col in self.target_columns_mapping_eng:\n",
        "                try:\n",
        "                    df[col] = df[col].replace([' ', ''], 0)\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "                    target_found.append(col)\n",
        "                    print(f\"íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬ ì™„ë£Œ: {col}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬ ì‹¤íŒ¨ {col}: {e}\")\n",
        "\n",
        "        # ë‹¤ë¥¸ ìƒì¥ ë³€ìˆ˜ë“¤ë„ ì €ì¥ (ì¶”í›„ ê½ƒ ì˜ˆì¸¡ìš©)\n",
        "        other_found = []\n",
        "        for col in df.columns:\n",
        "            if col in self.other_growth_columns_eng:\n",
        "                try:\n",
        "                    df[col] = df[col].replace([' ', ''], 0)\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "                    other_found.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        df = df.dropna(subset=['week'])\n",
        "\n",
        "        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë§¤í•‘\n",
        "        rename_dict = {'week': 'week'}\n",
        "        for english_col, target_col in self.target_columns_mapping_eng.items():\n",
        "            if english_col in df.columns:\n",
        "                rename_dict[english_col] = target_col\n",
        "\n",
        "        for english_col, target_col in self.other_growth_columns_eng.items():\n",
        "            if english_col in df.columns:\n",
        "                rename_dict[english_col] = target_col\n",
        "\n",
        "        print(f\"ìƒìœ¡ ë°ì´í„° ì»¬ëŸ¼ ë§¤í•‘: {rename_dict}\")\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        target_cols = [self.target_columns_mapping_eng[col] for col in target_found]\n",
        "        other_cols_eng = [self.other_growth_columns_eng[col] for col in other_found]\n",
        "        keep_cols = ['week'] + target_cols + other_cols_eng\n",
        "        available_cols = [col for col in keep_cols if col in df.columns]\n",
        "\n",
        "        print(f\"ìƒìœ¡ ë°ì´í„° ìµœì¢… ì„ íƒ ì»¬ëŸ¼: {available_cols}\")\n",
        "        print(f\"ìƒìœ¡ ë°ì´í„° ìµœì¢… í¬ê¸°: {df.shape}\")\n",
        "\n",
        "        if len(available_cols) <= 1:  # week ì»¬ëŸ¼ë§Œ ìˆëŠ” ê²½ìš°\n",
        "            print(f\"âŒ ë†ì¥ {farm_id}: ìœ íš¨í•œ ìƒìœ¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = df[available_cols]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _merge_farm_data(self, env_df, growth_df, farm_id):\n",
        "        \"\"\"ë†ì¥ë³„ í™˜ê²½-ìƒìœ¡ ë°ì´í„° ë³‘í•©\"\"\"\n",
        "        print(f\"ë†ì¥ {farm_id} ë°ì´í„° ë³‘í•© ì‹œì‘\")\n",
        "        print(f\"í™˜ê²½ ë°ì´í„°: {env_df.shape}, ì»¬ëŸ¼: {list(env_df.columns)}\")\n",
        "        print(f\"ìƒìœ¡ ë°ì´í„°: {growth_df.shape}, ì»¬ëŸ¼: {list(growth_df.columns)}\")\n",
        "\n",
        "        if env_df.empty or growth_df.empty:\n",
        "            print(f\"âŒ ë†ì¥ {farm_id}: í™˜ê²½ ë˜ëŠ” ìƒìœ¡ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # ì£¼ì°¨ë³„ ë³‘í•©\n",
        "        merged = pd.merge(env_df, growth_df, on='week', how='inner')\n",
        "        print(f\"ë³‘í•© ê²°ê³¼: {merged.shape}, ì»¬ëŸ¼: {list(merged.columns)}\")\n",
        "\n",
        "        if merged.empty:\n",
        "            print(f\"âŒ ë†ì¥ {farm_id}: ì£¼ì°¨ ë§¤ì¹­ ì‹¤íŒ¨\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # ë†ì¥ ID ì¶”ê°€\n",
        "        merged['farm_id'] = farm_id\n",
        "\n",
        "        print(f\"âœ… ë†ì¥ {farm_id} ë³‘í•© ì™„ë£Œ: {merged.shape}\")\n",
        "        return merged\n",
        "\n",
        "    def split_farms_for_validation(self, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_state=42):\n",
        "        \"\"\"ë†ì¥ì„ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• \"\"\"\n",
        "        farm_ids = list(self.farm_data.keys())\n",
        "        n_farms = len(farm_ids)\n",
        "\n",
        "        if n_farms < 3:\n",
        "            print(\"âŒ ìµœì†Œ 3ê°œ ë†ì¥ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "            return\n",
        "\n",
        "        np.random.seed(random_state)\n",
        "        shuffled_farms = np.random.permutation(farm_ids)\n",
        "\n",
        "        n_train = max(1, int(n_farms * train_ratio))\n",
        "        n_val = max(1, int(n_farms * val_ratio))\n",
        "        n_test = n_farms - n_train - n_val\n",
        "\n",
        "        self.train_farms = shuffled_farms[:n_train].tolist()\n",
        "        self.val_farms = shuffled_farms[n_train:n_train+n_val].tolist()\n",
        "        self.test_farms = shuffled_farms[n_train+n_val:].tolist()\n",
        "\n",
        "        print(f\"\\nğŸ¯ ë†ì¥ ë¶„í•  ê²°ê³¼:\")\n",
        "        print(f\"   í›ˆë ¨ìš©: {len(self.train_farms)}ê°œ ë†ì¥ - {self.train_farms}\")\n",
        "        print(f\"   ê²€ì¦ìš©: {len(self.val_farms)}ê°œ ë†ì¥ - {self.val_farms}\")\n",
        "        print(f\"   í…ŒìŠ¤íŠ¸ìš©: {len(self.test_farms)}ê°œ ë†ì¥ - {self.test_farms}\")\n",
        "\n",
        "    def prepare_datasets(self):\n",
        "        \"\"\"í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\"\"\"\n",
        "        if not self.train_farms:\n",
        "            print(\"âŒ ë¨¼ì € split_farms_for_validation()ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(\"\\nğŸ“¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...\")\n",
        "\n",
        "        # ê° ì„¸íŠ¸ë³„ ë°ì´í„° í†µí•©\n",
        "        train_data = pd.concat([self.farm_data[farm] for farm in self.train_farms], ignore_index=True)\n",
        "        val_data = pd.concat([self.farm_data[farm] for farm in self.val_farms], ignore_index=True)\n",
        "        test_data = pd.concat([self.farm_data[farm] for farm in self.test_farms], ignore_index=True)\n",
        "\n",
        "        print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}í–‰ ({len(self.train_farms)}ê°œ ë†ì¥)\")\n",
        "        print(f\"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data)}í–‰ ({len(self.val_farms)}ê°œ ë†ì¥)\")\n",
        "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}í–‰ ({len(self.test_farms)}ê°œ ë†ì¥)\")\n",
        "\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def add_lag_features(self, df, lag_weeks=[1, 2]):\n",
        "        \"\"\"ì‹œê°„ ì§€ì—° íŠ¹ì„± ì¶”ê°€\"\"\"\n",
        "        print(f\"â° ì§€ì—° íŠ¹ì„± ì¶”ê°€: {lag_weeks}ì£¼ ì§€ì—°\")\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì§€ì—° íŠ¹ì„± ìƒì„±\n",
        "        env_cols = [col for col in df.columns if col.startswith(('internal_', 'external_'))]\n",
        "\n",
        "        # ë†ì¥ë³„ë¡œ ì§€ì—° íŠ¹ì„± ìƒì„± (ë†ì¥ ê°„ ë°ì´í„° ì„ì„ ë°©ì§€)\n",
        "        df_with_lag = []\n",
        "\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].copy().sort_values('week')\n",
        "\n",
        "            for lag in lag_weeks:\n",
        "                for col in env_cols:\n",
        "                    lag_col_name = f\"{col}_lag{lag}\"\n",
        "                    farm_df[lag_col_name] = farm_df[col].shift(lag)\n",
        "\n",
        "            # ì§€ì—° íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ ê²°ì¸¡ì¹˜ ì œê±°\n",
        "            farm_df = farm_df.dropna()\n",
        "            df_with_lag.append(farm_df)\n",
        "\n",
        "        result_df = pd.concat(df_with_lag, ignore_index=True)\n",
        "\n",
        "        lag_cols_count = len([col for col in result_df.columns if 'lag' in col])\n",
        "        print(f\"âœ… ì§€ì—° íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ: {lag_cols_count}ê°œ\")\n",
        "\n",
        "        return result_df\n",
        "\n",
        "    def add_derived_features(self, df):\n",
        "        \"\"\"íŒŒìƒ ë³€ìˆ˜ ìƒì„±\"\"\"\n",
        "        print(\"ğŸ”§ íŒŒìƒ ë³€ìˆ˜ ìƒì„±...\")\n",
        "\n",
        "        # ì˜¨ë„ ì°¨ì´\n",
        "        if 'external_temp' in df.columns and 'internal_temp' in df.columns:\n",
        "            df['temp_diff'] = df['external_temp'] - df['internal_temp']\n",
        "\n",
        "        # ì¼ì‚¬ íš¨ìœ¨\n",
        "        if 'internal_solar' in df.columns and 'external_solar' in df.columns:\n",
        "            df['solar_efficiency'] = np.where(\n",
        "                df['external_solar'] > 0,\n",
        "                df['internal_solar'] / df['external_solar'],\n",
        "                0\n",
        "            )\n",
        "\n",
        "        # ì˜¨ìŠµë„ ì§€ìˆ˜\n",
        "        if 'internal_temp' in df.columns and 'internal_humidity' in df.columns:\n",
        "            df['temp_humidity_index'] = df['internal_temp'] * df['internal_humidity']\n",
        "\n",
        "        # ê³„ì ˆì„± íŠ¹ì„±\n",
        "        df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
        "        df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
        "\n",
        "        print(\"âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ\")\n",
        "        return df\n",
        "\n",
        "    def normalize_data(self, train_df, val_df, test_df):\n",
        "        \"\"\"ë°ì´í„° ì •ê·œí™” (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§)\"\"\"\n",
        "        print(\"ğŸ“ ë°ì´í„° ì •ê·œí™”...\")\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ë“¤\n",
        "        env_cols = [col for col in train_df.columns\n",
        "                   if col.startswith(('internal_', 'external_')) or\n",
        "                      'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                      'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ë“¤ - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        target_candidates = ['leaf_number', 'growth_length']\n",
        "        available_targets = [col for col in target_candidates if col in train_df.columns]\n",
        "\n",
        "        # ë§Œì•½ ì˜ë¬¸ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ í•œê¸€ ì»¬ëŸ¼ í™•ì¸\n",
        "        if not available_targets:\n",
        "            korean_targets = ['ì—½ìˆ˜(ê°œ)', 'ìƒì¥ê¸¸ì´(mm)']\n",
        "            available_targets = [col for col in korean_targets if col in train_df.columns]\n",
        "\n",
        "        # í™˜ê²½ ë³€ìˆ˜ ì •ê·œí™”\n",
        "        if env_cols:\n",
        "            train_df[env_cols] = self.scaler_env.fit_transform(train_df[env_cols])\n",
        "            val_df[env_cols] = self.scaler_env.transform(val_df[env_cols])\n",
        "            test_df[env_cols] = self.scaler_env.transform(test_df[env_cols])\n",
        "            print(f\"âœ… í™˜ê²½ ë³€ìˆ˜ ì •ê·œí™”: {len(env_cols)}ê°œ\")\n",
        "\n",
        "        # íƒ€ê²Ÿ ë³€ìˆ˜ ì •ê·œí™”\n",
        "        if available_targets:\n",
        "            train_df[available_targets] = self.scaler_growth.fit_transform(train_df[available_targets])\n",
        "            val_df[available_targets] = self.scaler_growth.transform(val_df[available_targets])\n",
        "            test_df[available_targets] = self.scaler_growth.transform(test_df[available_targets])\n",
        "            print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ì •ê·œí™”: {len(available_targets)}ê°œ - {available_targets}\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    def create_sequences(self, df, sequence_length=3):\n",
        "        \"\"\"GRUìš© ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ë†ì¥ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬)\"\"\"\n",
        "        print(f\"ğŸ”„ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ê¸¸ì´: {sequence_length}ì£¼)\")\n",
        "        print(f\"ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ë“¤: {list(df.columns)}\")  # ë””ë²„ê¹…ìš©\n",
        "\n",
        "        # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col.startswith(('internal_', 'external_')) or\n",
        "                          'lag' in col or 'temp_diff' in col or 'solar_efficiency' in col or\n",
        "                          'temp_humidity_index' in col or 'week_sin' in col or 'week_cos' in col]\n",
        "\n",
        "        # íƒ€ê²Ÿ ì»¬ëŸ¼ - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
        "        target_candidates = ['leaf_number', 'growth_length']\n",
        "        available_targets = [col for col in target_candidates if col in df.columns]\n",
        "\n",
        "        # ë§Œì•½ ì˜ë¬¸ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ í•œê¸€ ì»¬ëŸ¼ í™•ì¸\n",
        "        if not available_targets:\n",
        "            korean_targets = ['ì—½ìˆ˜(ê°œ)', 'ìƒì¥ê¸¸ì´(mm)']\n",
        "            available_targets = [col for col in korean_targets if col in df.columns]\n",
        "            print(f\"í•œê¸€ íƒ€ê²Ÿ ì»¬ëŸ¼ í™•ì¸: {available_targets}\")\n",
        "\n",
        "        if not available_targets:\n",
        "            print(f\"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            print(f\"   ì°¾ê³  ìˆëŠ” ì»¬ëŸ¼: {target_candidates}\")\n",
        "            print(f\"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"ğŸ“Š íŠ¹ì„± ë³€ìˆ˜: {len(feature_cols)}ê°œ - {feature_cols[:5]}...\")\n",
        "        print(f\"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {len(available_targets)}ê°œ - {available_targets}\")\n",
        "\n",
        "        # ë†ì¥ë³„ë¡œ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "        X_list, y_list = [], []\n",
        "\n",
        "        for farm_id in df['farm_id'].unique():\n",
        "            farm_df = df[df['farm_id'] == farm_id].sort_values('week')\n",
        "\n",
        "            if len(farm_df) <= sequence_length:\n",
        "                continue\n",
        "\n",
        "            for i in range(sequence_length, len(farm_df)):\n",
        "                # ê³¼ê±° sequence_length ì£¼ê°„ì˜ í™˜ê²½ ë°ì´í„°\n",
        "                X_list.append(farm_df[feature_cols].iloc[i-sequence_length:i].values)\n",
        "                # í˜„ì¬ ì£¼ì˜ íƒ€ê²Ÿ ê°’\n",
        "                y_list.append(farm_df[available_targets].iloc[i].values)\n",
        "\n",
        "        if not X_list:\n",
        "            print(\"âŒ ìƒì„±ëœ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return None, None\n",
        "\n",
        "        X = np.array(X_list)\n",
        "        y = np.array(y_list)\n",
        "\n",
        "        print(f\"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
        "        print(f\"   - X shape: {X.shape} (samples, time_steps, features)\")\n",
        "        print(f\"   - y shape: {y.shape} (samples, targets)\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def save_processed_data(self, train_df, val_df, test_df, save_path='/content/drive/MyDrive/mod/processed'):\n",
        "        \"\"\"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\"\"\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # CSVë¡œ ì €ì¥\n",
        "        train_df.to_csv(f\"{save_path}/train_data.csv\", index=False, encoding='utf-8-sig')\n",
        "        val_df.to_csv(f\"{save_path}/val_data.csv\", index=False, encoding='utf-8-sig')\n",
        "        test_df.to_csv(f\"{save_path}/test_data.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
        "        import joblib\n",
        "        joblib.dump(self.scaler_env, f\"{save_path}/scaler_env.pkl\")\n",
        "        joblib.dump(self.scaler_growth, f\"{save_path}/scaler_growth.pkl\")\n",
        "\n",
        "        # ë†ì¥ ë¶„í•  ì •ë³´ ì €ì¥\n",
        "        farm_split_info = {\n",
        "            'train_farms': self.train_farms,\n",
        "            'val_farms': self.val_farms,\n",
        "            'test_farms': self.test_farms\n",
        "        }\n",
        "        import json\n",
        "        with open(f\"{save_path}/farm_split.json\", 'w') as f:\n",
        "            json.dump(farm_split_info, f, indent=2)\n",
        "\n",
        "        # ë°ì´í„° ì •ë³´ ì €ì¥\n",
        "        with open(f\"{save_path}/data_info.txt\", 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ì „ì²˜ë¦¬ ë°ì´í„° ì •ë³´\\n\")\n",
        "            f.write(f\"ìƒì„± ì¼ì‹œ: {pd.Timestamp.now()}\\n\\n\")\n",
        "            f.write(f\"ë†ì¥ ë¶„í• :\\n\")\n",
        "            f.write(f\"  í›ˆë ¨: {self.train_farms}\\n\")\n",
        "            f.write(f\"  ê²€ì¦: {self.val_farms}\\n\")\n",
        "            f.write(f\"  í…ŒìŠ¤íŠ¸: {self.test_farms}\\n\\n\")\n",
        "            f.write(f\"ë°ì´í„° í¬ê¸°:\\n\")\n",
        "            f.write(f\"  í›ˆë ¨: {train_df.shape}\\n\")\n",
        "            f.write(f\"  ê²€ì¦: {val_df.shape}\\n\")\n",
        "            f.write(f\"  í…ŒìŠ¤íŠ¸: {test_df.shape}\\n\\n\")\n",
        "            f.write(f\"íƒ€ê²Ÿ ë³€ìˆ˜: ì—½ìˆ˜(leaf_number), ìƒì¥ê¸¸ì´(growth_length)\\n\")\n",
        "            f.write(f\"ì œì™¸ëœ ì„¼ì„œ: {self.excluded_env_sensors}\\n\")\n",
        "\n",
        "        print(f\"\\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "    def plot_farm_overview(self, train_df, val_df, test_df):\n",
        "        \"\"\"ë†ì¥ë³„ ë°ì´í„° ê°œìš” ì‹œê°í™”\"\"\"\n",
        "        print(\"ğŸ“Š ë†ì¥ë³„ ë°ì´í„° ì‹œê°í™”...\")\n",
        "\n",
        "        # ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê²Ÿ ì»¬ëŸ¼ í™•ì¸\n",
        "        target_candidates = ['leaf_number', 'growth_length', 'ì—½ìˆ˜(ê°œ)', 'ìƒì¥ê¸¸ì´(mm)']\n",
        "        available_targets = []\n",
        "        for col in target_candidates:\n",
        "            if col in train_df.columns:\n",
        "                available_targets.append(col)\n",
        "\n",
        "        if len(available_targets) < 2:\n",
        "            print(f\"âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(train_df.columns)}\")\n",
        "            return\n",
        "\n",
        "        # ë†ì¥ë³„ ë°ì´í„° ë¶„í¬\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # 1. ë†ì¥ë³„ ë°ì´í„° ê°œìˆ˜\n",
        "        all_data = pd.concat([\n",
        "            train_df.assign(split='Train'),\n",
        "            val_df.assign(split='Validation'),\n",
        "            test_df.assign(split='Test')\n",
        "        ])\n",
        "\n",
        "        farm_counts = all_data.groupby(['farm_id', 'split']).size().unstack(fill_value=0)\n",
        "        farm_counts.plot(kind='bar', stacked=True, ax=axes[0,0])\n",
        "        axes[0,0].set_title('ë†ì¥ë³„ ë°ì´í„° ë¶„í¬')\n",
        "        axes[0,0].set_xlabel('ë†ì¥ ID')\n",
        "        axes[0,0].set_ylabel('ë°ì´í„° ê°œìˆ˜')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. ì²« ë²ˆì§¸ íƒ€ê²Ÿ ë¶„í¬\n",
        "        target1 = available_targets[0]\n",
        "        axes[0,1].hist([train_df[target1], val_df[target1], test_df[target1]],\n",
        "                      bins=20, alpha=0.7, label=['Train', 'Val', 'Test'])\n",
        "        axes[0,1].set_title(f'{target1} ë¶„í¬')\n",
        "        axes[0,1].set_xlabel(target1)\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # 3. ë‘ ë²ˆì§¸ íƒ€ê²Ÿ ë¶„í¬\n",
        "        target2 = available_targets[1]\n",
        "        axes[1,0].hist([train_df[target2], val_df[target2], test_df[target2]],\n",
        "                      bins=20, alpha=0.7, label=['Train', 'Val', 'Test'])\n",
        "        axes[1,0].set_title(f'{target2} ë¶„í¬')\n",
        "        axes[1,0].set_xlabel(target2)\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 4. ì£¼ì°¨ë³„ í‰ê·  íƒ€ê²Ÿ ê°’\n",
        "        week_stats = all_data.groupby('week')[available_targets[:2]].mean()\n",
        "        week_stats.plot(ax=axes[1,1])\n",
        "        axes[1,1].set_title('ì£¼ì°¨ë³„ í‰ê·  íƒ€ê²Ÿ ê°’')\n",
        "        axes[1,1].set_xlabel('ì£¼ì°¨')\n",
        "        axes[1,1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def main():\n",
        "    \"\"\"14ê°œ ë†ì¥ ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
        "    print(\"ğŸŒ± 14ê°œ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘\")\n",
        "    print(\"ğŸ¯ íƒ€ê²Ÿ: ì—½ìˆ˜(leaf_number) + ìƒì¥ê¸¸ì´(growth_length)\")\n",
        "    print(\"ğŸš« ì œì™¸ ì„¼ì„œ: ì§€ì˜¨, í’í–¥, í’ì†\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”\n",
        "    processor = SmartFarm14DataProcessor()\n",
        "\n",
        "    # 1. 14ê°œ ë†ì¥ ë°ì´í„° ë¡œë“œ\n",
        "    success_count = processor.load_all_farm_data()\n",
        "\n",
        "    if success_count < 3:\n",
        "        print(\"âŒ ìµœì†Œ 3ê°œ ë†ì¥ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # 2. ë†ì¥ì„ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• \n",
        "    processor.split_farms_for_validation(train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)\n",
        "\n",
        "    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "    train_data, val_data, test_data = processor.prepare_datasets()\n",
        "\n",
        "    # 4. ì§€ì—° íŠ¹ì„± ì¶”ê°€\n",
        "    train_data = processor.add_lag_features(train_data, lag_weeks=[1, 2])\n",
        "    val_data = processor.add_lag_features(val_data, lag_weeks=[1, 2])\n",
        "    test_data = processor.add_lag_features(test_data, lag_weeks=[1, 2])\n",
        "\n",
        "    # 5. íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
        "    train_data = processor.add_derived_features(train_data)\n",
        "    val_data = processor.add_derived_features(val_data)\n",
        "    test_data = processor.add_derived_features(test_data)\n",
        "\n",
        "    # 6. ë°ì´í„° ì •ê·œí™”\n",
        "    train_data, val_data, test_data = processor.normalize_data(train_data, val_data, test_data)\n",
        "\n",
        "    # 7. ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    X_train, y_train = processor.create_sequences(train_data, sequence_length=3)\n",
        "    X_val, y_val = processor.create_sequences(val_data, sequence_length=3)\n",
        "    X_test, y_test = processor.create_sequences(test_data, sequence_length=3)\n",
        "\n",
        "    # 8. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
        "    processor.save_processed_data(train_data, val_data, test_data)\n",
        "\n",
        "    # 9. ë°ì´í„° ê°œìš” ì‹œê°í™”\n",
        "    processor.plot_farm_overview(train_data, val_data, test_data)\n",
        "\n",
        "    print(\"\\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š ìµœì¢… ì‹œí€€ìŠ¤ ë°ì´í„°:\")\n",
        "    print(f\"   í›ˆë ¨: X{X_train.shape if X_train is not None else 'None'}, y{y_train.shape if y_train is not None else 'None'}\")\n",
        "    print(f\"   ê²€ì¦: X{X_val.shape if X_val is not None else 'None'}, y{y_val.shape if y_val is not None else 'None'}\")\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸: X{X_test.shape if X_test is not None else 'None'}, y{y_test.shape if y_test is not None else 'None'}\")\n",
        "\n",
        "    return processor, (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    processor, train_data, val_data, test_data = main()"
      ],
      "metadata": {
        "id": "-7C4OmyXO10r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨\n",
        "print(\"ğŸŒ± ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ì‹œì‘...\")\n",
        "model_cls, trainer_cls, accuracy, class_names = main_classification_training(processor_new)\n",
        "\n",
        "# 2. ê²°ê³¼ ìš”ì•½\n",
        "print(f\"\\nğŸ‰ ì—½ìˆ˜ ë¶„ë¥˜ ëª¨ë¸ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Š ìµœì¢… ì„±ëŠ¥:\")\n",
        "print(f\"   ì •í™•ë„: {accuracy:.1%}\")\n",
        "print(f\"   í´ë˜ìŠ¤: {class_names}\")\n",
        "\n",
        "# 3. íšŒê·€ vs ë¶„ë¥˜ ì„±ëŠ¥ ë¹„êµ\n",
        "print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "print(f\"   íšŒê·€ ëª¨ë¸ (RÂ²): 0.095 (9.5%) âŒ\")\n",
        "print(f\"   ë¶„ë¥˜ ëª¨ë¸ (Accuracy): {accuracy:.1%} âœ…\")\n",
        "print(f\"   ê°œì„ ë„: {accuracy/0.095:.1f}ë°° í–¥ìƒ!\")"
      ],
      "metadata": {
        "id": "o1zGrWBiO4v1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}