{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwn/xfmFSejsDxnoTlkAQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungwoosoon/SmartFarmProject/blob/AI/condition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG1ZhjBDpCgg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76957d3"
      },
      "source": [
        "# Task\n",
        "Load the CSV file \"MyDrive/asset/condtion/condtion1.csv\", fill missing values using linear interpolation, and remove the columns '시작', '종료', '누적량1', '일출시간', '일몰시간', 'EC급액', 'EC배액', 'pH급액', '저울급액', '저울배액', '온도 급액', '내부 이산화탄소', '내부광량', '내부PT100온도센서1', '내부 PT100온도센서2', '내부 PT100온도센서3', '내부PT100센서를 이용', and '설정pH'. Then, explain how to predict weekly tomato fruit set based on weekly environmental data and weekly growth indicators, considering the current time difference between the time series data and growth indicators by aggregating data weekly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddc11a65"
      },
      "source": [
        "## 데이터 준비\n",
        "\n",
        "### Subtask:\n",
        "환경 데이터와 생장 지표 데이터를 불러오고 시간 정보를 주차 정보로 변환합니다. (필요하다면 이전 단계에서 처리된 환경 데이터를 사용합니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788fab34"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step did not load the data, so the first step is to load the data from the specified CSV file into a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84d08a6b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the data failed due to a `FileNotFoundError`. This indicates the file path was incorrect. I will try a different path to load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25bd6a01"
      },
      "source": [
        "## 데이터 준비\n",
        "\n",
        "### Subtask:\n",
        "환경 데이터와 생장 지표 데이터를 불러오고 시간 정보를 주차 정보로 변환합니다. (필요하다면 이전 단계에서 처리된 환경 데이터를 사용합니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd59c11e"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the CSV file again, this time trying to find the correct path. If the file is still not found, it indicates a persistent issue with the file path or existence, and I will report failure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcef576"
      },
      "source": [
        "## 데이터 준비\n",
        "\n",
        "### Subtask:\n",
        "환경 데이터와 생장 지표 데이터를 불러오고 시간 정보를 주차 정보로 변환합니다. (필요하다면 이전 단계에서 처리된 환경 데이터를 사용합니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d56231a7"
      },
      "source": [
        "**Reasoning**:\n",
        "Attempt to load the data from the specified path and the current directory, handle missing values with linear interpolation, remove the specified columns, and display the head of the dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ead90f8"
      },
      "source": [
        "## 주간 데이터 집계\n",
        "\n",
        "### Subtask:\n",
        "환경 데이터와 생장 지표 데이터를 주차별로 집계합니다. 환경 데이터는 평균값을, 생장 지표는 주차 마지막 값을 사용하거나 다른 적절한 집계 방법을 선택합니다. 토마토 착과수는 주차별 총합을 계산합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2249c3"
      },
      "source": [
        "## 주간 데이터 집계\n",
        "\n",
        "### Subtask:\n",
        "환경 데이터와 생장 지표 데이터를 주차별로 집계합니다. 환경 데이터는 평균값을, 생장 지표는 주차 마지막 값을 사용하거나 다른 적절한 집계 방법을 선택합니다. 토마토 착과수는 주차별 총합을 계산합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "366cd342"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous attempts, the dataframes `df_growth` and `df_fruit_set` are not available. I will proceed with aggregating the available `df_condition` by '주차' using the mean, as requested in the instructions. I will then display the head of the resulting dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "242ed281"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1089952d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/asset/condition/condition1.csv'\n",
        "\n",
        "try:\n",
        "    df_condition = pd.read_csv(file_path)\n",
        "\n",
        "    # Fill missing values using linear interpolation\n",
        "    df_condition.interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Remove specified columns\n",
        "    columns_to_drop = ['시작', '종료', '누적량1', '일출시간', '일몰시간', 'EC급액', 'EC배액', 'pH급액', '저울급액', '저울배액', '온도 급액', '내부 이산화탄소', '내부광량', '내부PT100온도센서1', '내부 PT100온도센서2', '내부 PT100온도센서3', '내부PT100센서를 이용', '설정pH']\n",
        "    df_condition.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
        "\n",
        "    print(\"Data loaded, missing values filled, and specified columns dropped.\")\n",
        "    display(df_condition.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {file_path}. Please check the path and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01ecd749"
      },
      "source": [
        "# Convert 'date' column to datetime objects\n",
        "df_condition['date'] = pd.to_datetime(df_condition['date'])\n",
        "\n",
        "# Set 'date' as the index\n",
        "df_condition.set_index('date', inplace=True)\n",
        "\n",
        "# Resample the data weekly and calculate the mean for environmental data\n",
        "df_weekly_condition = df_condition.resample('W').mean()\n",
        "\n",
        "# Assuming '토마토 착과수' column exists in df_condition, calculate the weekly sum\n",
        "# If '토마토 착과수' is in a separate dataframe, load and merge it first.\n",
        "# For now, let's assume it's not in df_condition and needs to be loaded separately.\n",
        "# I will proceed with only the weekly environmental data aggregation.\n",
        "\n",
        "print(\"Weekly environmental data aggregated.\")\n",
        "display(df_weekly_condition.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5df95c2f"
      },
      "source": [
        "# Select relevant columns based on user's request\n",
        "selected_columns = ['현재EC(dS)', '현재PH(pH)', '현재일사(W)', '누적일사(J)', '내부이산화탄소', '내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번', '내부PT100센서를이용한계산습도']\n",
        "df_selected_env = df_weekly_condition[selected_columns].copy()\n",
        "\n",
        "# Calculate the average of the three temperature sensor columns\n",
        "temperature_columns = ['내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번']\n",
        "df_selected_env['평균온도'] = df_selected_env[temperature_columns].mean(axis=1)\n",
        "\n",
        "# Format the average temperature to one decimal place\n",
        "df_selected_env['평균온도'] = df_selected_env['평균온도'].round(1)\n",
        "\n",
        "# Drop the original individual temperature columns\n",
        "df_selected_env.drop(columns=temperature_columns, inplace=True)\n",
        "\n",
        "# Interpolate missing values in the '내부이산화탄소' column\n",
        "df_selected_env['내부이산화탄소'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "\n",
        "print(\"Selected environmental data with average temperature and interpolated CO2 values:\")\n",
        "display(df_selected_env.head())\n",
        "\n",
        "print(\"\\n'내부이산화탄소' 칼럼의 결측치 개수 (보간 후):\")\n",
        "display(df_selected_env['내부이산화탄소'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6b4bd7c"
      },
      "source": [
        "print(\"결측치 여부 (True: 결측치 있음):\")\n",
        "display(df_selected_env['내부이산화탄소'].isnull().any())\n",
        "\n",
        "print(\"\\n'내부이산화탄소' 칼럼의 결측치 개수:\")\n",
        "display(df_selected_env['내부이산화탄소'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20f91b08"
      },
      "source": [
        "# Drop rows with any missing values\n",
        "df_cleaned_env = df_selected_env.dropna()\n",
        "\n",
        "print(\"결측치가 포함된 행 제거 후 데이터:\")\n",
        "display(df_cleaned_env.head())\n",
        "\n",
        "print(\"\\n결측치 개수 (모든 칼럼):\")\n",
        "display(df_cleaned_env.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a72e83ec"
      },
      "source": [
        "# Task\n",
        "Analyze the provided time-series environmental data from \"MyDrive/asset/condtion/condition1.csv\", which has been preprocessed to handle missing values and select relevant columns (온도, 일사량, 습도, 이산화탄소 농도, pH, 현재 일사, 누적 일사, 내부 PT100온도센서 평균). Combine this with weekly growth data (assuming it is available or will be provided) to predict weekly tomato growth (CM) using an MLP model. The environmental data should be aggregated into 3-day intervals for statistical analysis before being used with the weekly growth data. Provide the steps to prepare the data, build and train the MLP model, and evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40296dc9"
      },
      "source": [
        "## 주간 데이터 준비\n",
        "\n",
        "### Subtask:\n",
        "주간 환경 데이터 (`df_cleaned_env`)와 주간 생장 지표 데이터를 준비합니다. (주간 생장 지표 데이터가 없다면 불러오거나 주간으로 집계합니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c7b53a"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if the DataFrame `df_growth_weekly` exists. If it does, display its head; otherwise, explain that the data is missing and describe the expected format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7591ae34"
      },
      "source": [
        "try:\n",
        "    # Check if df_growth_weekly exists\n",
        "    if 'df_growth_weekly' in locals():\n",
        "        print(\"Weekly growth indicator data found.\")\n",
        "        display(df_growth_weekly.head())\n",
        "    else:\n",
        "        print(\"Weekly growth indicator data (df_growth_weekly) not found.\")\n",
        "        print(\"This data is needed for the next steps.\")\n",
        "        print(\"Expected format: A DataFrame with a time identifier column (e.g., '주차' or a datetime index) and a target variable column ('주간생장량(CM)').\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while checking for weekly growth data: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbffa0e"
      },
      "source": [
        "## 데이터 병합\n",
        "\n",
        "### Subtask:\n",
        "주간 환경 데이터와 주간 생장 지표 데이터를 주차 또는 날짜를 기준으로 병합하여 최종 학습 데이터셋을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e475c6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step indicated that the weekly growth data was missing. To proceed with merging the environmental and growth data, I need to load a placeholder weekly growth dataset with a compatible time identifier and a target variable column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2144c72a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a placeholder DataFrame for weekly growth data\n",
        "# Assuming the weekly growth data has a 'date' column and a '주간생장량(CM)' column\n",
        "# The dates should align with or overlap with the dates in df_cleaned_env\n",
        "\n",
        "# Generate sample dates (weekly) that align with df_cleaned_env index\n",
        "start_date = df_cleaned_env.index.min()\n",
        "end_date = df_cleaned_env.index.max()\n",
        "weekly_dates = pd.date_range(start=start_date, end=end_date, freq='W')\n",
        "\n",
        "# Generate random placeholder growth data\n",
        "np.random.seed(42) # for reproducibility\n",
        "weekly_growth_cm = np.random.rand(len(weekly_dates)) * 10 # Random growth between 0 and 10 cm\n",
        "\n",
        "df_growth_weekly = pd.DataFrame({'date': weekly_dates, '주간생장량(CM)': weekly_growth_cm})\n",
        "\n",
        "# Set 'date' as the index to match df_cleaned_env\n",
        "df_growth_weekly.set_index('date', inplace=True)\n",
        "\n",
        "print(\"Placeholder weekly growth data created:\")\n",
        "display(df_growth_weekly.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03c80dd4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `df_cleaned_env` was not defined. I need to redefine `df_cleaned_env` by repeating the data loading, cleaning, and aggregation steps from the previous successful execution, and then proceed with creating the placeholder weekly growth data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHpN5vecyBvR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Repeat data loading and cleaning steps to define df_cleaned_env\n",
        "file_path = '/content/drive/MyDrive/asset/condition/condition1.csv'\n",
        "\n",
        "try:\n",
        "    df_condition = pd.read_csv(file_path)\n",
        "\n",
        "    # Fill missing values using linear interpolation\n",
        "    df_condition.interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Remove specified columns\n",
        "    columns_to_drop = ['시작', '종료', '누적량1', '일출시간', '일몰시간', 'EC급액', 'EC배액', 'pH급액', '저울급액', '저울배액', '온도 급액', '내부 이산화탄소', '내부광량', '내부PT100온도센서1', '내부 PT100온도센서2', '내부 PT100온도센서3', '내부PT100센서를 이용', '설정pH']\n",
        "    df_condition.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
        "\n",
        "    # Convert 'date' column to datetime objects\n",
        "    df_condition['date'] = pd.to_datetime(df_condition['date'])\n",
        "\n",
        "    # Set 'date' as the index\n",
        "    df_condition.set_index('date', inplace=True)\n",
        "\n",
        "    # Resample the data weekly and calculate the mean for environmental data\n",
        "    df_weekly_condition = df_condition.resample('W').mean()\n",
        "\n",
        "    # Select relevant columns\n",
        "    selected_columns = ['현재EC(dS)', '현재PH(pH)', '현재일사(W)', '누적일사(J)', '내부이산화탄소', '내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번', '내부PT100센서를 이용한 계산 습도']\n",
        "    df_selected_env = df_weekly_condition[selected_columns].copy()\n",
        "\n",
        "    # Calculate the average of the three temperature sensor columns\n",
        "    temperature_columns = ['내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번']\n",
        "    df_selected_env['평균온도'] = df_selected_env[temperature_columns].mean(axis=1)\n",
        "\n",
        "    # Format the average temperature to one decimal place\n",
        "    df_selected_env['평균온도'] = df_selected_env['평균온도'].round(1)\n",
        "\n",
        "    # Drop the original individual temperature columns\n",
        "    df_selected_env.drop(columns=temperature_columns, inplace=True)\n",
        "\n",
        "    # Interpolate missing values in the '내부이산화탄소' column\n",
        "    df_selected_env['내부이산화탄소'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Drop rows with any missing values\n",
        "    df_cleaned_env = df_selected_env.dropna()\n",
        "\n",
        "    print(\"Environmental data loaded, cleaned, aggregated weekly, and missing values handled.\")\n",
        "    display(df_cleaned_env.head())\n",
        "\n",
        "    # Now, create the placeholder DataFrame for weekly growth data\n",
        "    # Generate sample dates (weekly) that align with df_cleaned_env index\n",
        "    start_date = df_cleaned_env.index.min()\n",
        "    end_date = df_cleaned_env.index.max()\n",
        "    weekly_dates = pd.date_range(start=start_date, end=end_date, freq='W')\n",
        "\n",
        "    # Generate random placeholder growth data\n",
        "    np.random.seed(42) # for reproducibility\n",
        "    weekly_growth_cm = np.random.rand(len(weekly_dates)) * 10 # Random growth between 0 and 10 cm\n",
        "\n",
        "    df_growth_weekly = pd.DataFrame({'date': weekly_dates, '주간생장량(CM)': weekly_growth_cm})\n",
        "\n",
        "    # Set 'date' as the index to match df_cleaned_env\n",
        "    df_growth_weekly.set_index('date', inplace=True)\n",
        "\n",
        "    print(\"\\nPlaceholder weekly growth data created:\")\n",
        "    display(df_growth_weekly.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {file_path}. Please check the path and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7877a172"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path_growth = '/content/drive/MyDrive/updated_sorted_data/tom1_updated_sorted_data.xlsx'\n",
        "\n",
        "try:\n",
        "    df_growth = pd.read_excel(file_path_growth)\n",
        "\n",
        "    print(\"주간 생장 데이터 로드 완료:\")\n",
        "    display(df_growth.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: 주간 생장 데이터 파일을 찾을 수 없습니다: {file_path_growth}. 경로를 확인해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"주간 생장 데이터 로드 중 오류 발생: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdd399e4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path_growth = '/content/drive/MyDrive/updated_sorted_data/tom1_updated_sorted_data.xlsx'\n",
        "\n",
        "try:\n",
        "    df_growth = pd.read_excel(file_path_growth)\n",
        "\n",
        "    print(\"주간 생장 데이터 로드 완료:\")\n",
        "    display(df_growth.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: 주간 생장 데이터 파일을 찾을 수 없습니다: {file_path_growth}. 경로를 확인해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"주간 생장 데이터 로드 중 오류 발생: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ffdb3d"
      },
      "source": [
        "# Assign correct column names based on the last row\n",
        "df_growth.columns = df_growth.iloc[-1]\n",
        "\n",
        "# Drop the last row which contains the column names\n",
        "df_growth = df_growth.iloc[:-1]\n",
        "\n",
        "# Convert the 'date' column to datetime objects\n",
        "df_growth['date'] = pd.to_datetime(df_growth['date'], format='%Y%m%d')\n",
        "\n",
        "# Set 'date' as the index\n",
        "df_growth.set_index('date', inplace=True)\n",
        "\n",
        "# Convert relevant columns to numeric, coercing errors\n",
        "df_growth['week'] = pd.to_numeric(df_growth['week'], errors='coerce')\n",
        "df_growth['weeklyGrowth'] = pd.to_numeric(df_growth['weeklyGrowth'], errors='coerce')\n",
        "\n",
        "# Drop rows where key columns like 'weeklyGrowth' became NaN after coercion\n",
        "df_growth.dropna(subset=['weeklyGrowth'], inplace=True)\n",
        "\n",
        "\n",
        "print(\"주간 생장 데이터 전처리 완료:\")\n",
        "display(df_growth.head())\n",
        "\n",
        "print(\"\\n전처리 후 데이터 정보:\")\n",
        "df_growth.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8db6eb35"
      },
      "source": [
        "# Merge the weekly environmental data and weekly growth dataframes on their index (date)\n",
        "df_merged = pd.merge(df_cleaned_env, df_growth[['weeklyGrowth']], left_index=True, right_index=True, how='inner')\n",
        "\n",
        "print(\"주간 환경 데이터와 생장 데이터 병합 완료:\")\n",
        "display(df_merged.head())\n",
        "\n",
        "print(\"\\n병합된 데이터 정보:\")\n",
        "df_merged.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1783c6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Repeat data loading and cleaning steps to define df_cleaned_env\n",
        "file_path = '/content/drive/MyDrive/asset/condition/condition1.csv'\n",
        "\n",
        "try:\n",
        "    df_condition = pd.read_csv(file_path)\n",
        "\n",
        "    # Fill missing values using linear interpolation\n",
        "    df_condition.interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Remove specified columns\n",
        "    columns_to_drop = ['시작', '종료', '누적량1', '일출시간', '일몰시간', 'EC급액', 'EC배액', 'pH급액', '저울급액', '저울배액', '온도 급액', '내부 이산화탄소', '내부광량', '내부PT100온도센서1', '내부 PT100온도센서2', '내부 PT100온도센서3', '내부PT100센서를 이용', '설정pH']\n",
        "    df_condition.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
        "\n",
        "    # Convert 'date' column to datetime objects\n",
        "    df_condition['date'] = pd.to_datetime(df_condition['date'])\n",
        "\n",
        "    # Set 'date' as the index\n",
        "    df_condition.set_index('date', inplace=True)\n",
        "\n",
        "    # Resample the data weekly and calculate the mean for environmental data\n",
        "    df_weekly_condition = df_condition.resample('W').mean()\n",
        "\n",
        "    # Select relevant columns\n",
        "    selected_columns = ['현재EC(dS)', '현재PH(pH)', '현재일사(W)', '누적일사(J)', '내부이산화탄소', '내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번', '내부PT100센서를이용한계산습도']\n",
        "    df_selected_env = df_weekly_condition[selected_columns].copy()\n",
        "\n",
        "    # Rename the humidity column\n",
        "    df_selected_env.rename(columns={'내부PT100센서를이용한계산습도': '습도'}, inplace=True)\n",
        "\n",
        "    # Calculate the average of the three temperature sensor columns\n",
        "    temperature_columns = ['내부PT100온도센서1번건구', '내부PT100온도센서2번습구', '내부PT100온도센서3번']\n",
        "    df_selected_env['평균온도'] = df_selected_env[temperature_columns].mean(axis=1)\n",
        "\n",
        "    # Format the average temperature to one decimal place\n",
        "    df_selected_env['평균온도'] = df_selected_env['평균온도'].round(1)\n",
        "\n",
        "    # Drop the original individual temperature columns\n",
        "    df_selected_env.drop(columns=temperature_columns, inplace=True)\n",
        "\n",
        "    # Interpolate missing values in the '내부이산화탄소' column\n",
        "    df_selected_env['내부이산화탄소'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "    # Drop rows with any missing values\n",
        "    df_cleaned_env = df_selected_env.dropna()\n",
        "\n",
        "    print(\"Environmental data loaded, cleaned, aggregated weekly, and missing values handled.\")\n",
        "    display(df_cleaned_env.head())\n",
        "\n",
        "    # Merge the weekly environmental data and weekly growth dataframes on their index (date)\n",
        "    # Assuming df_growth is already loaded and preprocessed\n",
        "    if 'df_growth' in locals():\n",
        "        df_merged = pd.merge(df_cleaned_env, df_growth[['weeklyGrowth']], left_index=True, right_index=True, how='inner')\n",
        "\n",
        "        print(\"\\n주간 환경 데이터와 생장 데이터 병합 완료:\")\n",
        "        display(df_merged.head())\n",
        "\n",
        "        print(\"\\n병합된 데이터 정보:\")\n",
        "        df_merged.info()\n",
        "    else:\n",
        "        print(\"\\nError: 주간 생장 데이터 (df_growth)가 로드되지 않았습니다. 생장 데이터를 먼저 로드해주세요.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {file_path}. Please check the path and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "431a0143"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "### Subtask:\n",
        "MLP 모델 학습을 위해 병합된 데이터를 전처리합니다. 특성 스케일링을 수행하고, 필요하다면 시계열 데이터의 특성을 고려한 추가 전처리를 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ccc744"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_merged.drop('weeklyGrowth', axis=1)\n",
        "y = df_merged['weeklyGrowth']\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Features scaled:\")\n",
        "display(X_scaled[:5])\n",
        "\n",
        "print(\"\\nTarget variable (first 5 values):\")\n",
        "display(y.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592487d0"
      },
      "source": [
        "## 데이터 분할\n",
        "\n",
        "### Subtask:\n",
        "학습 데이터셋을 훈련 세트와 테스트 세트로 분할합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "691ee854"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Using a standard split (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data split into training and testing sets:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82a463f0"
      },
      "source": [
        "## MLP 모델 구축 및 학습\n",
        "\n",
        "### Subtask:\n",
        "MLP 회귀 모델을 정의하고 훈련 데이터셋을 사용하여 학습시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5374d889"
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Define the MLP Regressor model\n",
        "# You can adjust the hidden_layer_sizes, activation, solver, etc.\n",
        "mlp_model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "\n",
        "# Train the model using the training data\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"MLP model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed2271c9"
      },
      "source": [
        "## 모델 평가\n",
        "\n",
        "### Subtask:\n",
        "학습된 MLP 모델의 성능을 테스트 데이터셋을 사용하여 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d471cad"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = mlp_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MLP Model Evaluation:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"R-squared (R2): {r2:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}